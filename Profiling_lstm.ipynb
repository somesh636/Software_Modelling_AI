{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Profiling_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyME8SPZ963PtVFVGfY8Vn1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somesh636/Software_Modelling_AI/blob/master/Profiling_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jPd7vxnmiwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "PyTorch Child-Sum Tree LSTM model\n",
        "See Tai et al. 2015 https://arxiv.org/abs/1503.00075 for model description.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class TreeLSTM(torch.nn.Module):\n",
        "    '''PyTorch TreeLSTM model that implements efficient batching.\n",
        "    '''\n",
        "    def __init__(self, in_features, out_features):\n",
        "        '''TreeLSTM class initializer\n",
        "        Takes in int sizes of in_features and out_features and sets up model Linear network layers.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # bias terms are only on the W layers for efficiency\n",
        "        self.W_iou = torch.nn.Linear(self.in_features, 3 * self.out_features)\n",
        "        self.U_iou = torch.nn.Linear(self.out_features, 3 * self.out_features, bias=False)\n",
        "\n",
        "        # f terms are maintained seperate from the iou terms because they involve sums over child nodes\n",
        "        # while the iou terms do not\n",
        "        self.W_f = torch.nn.Linear(self.in_features, self.out_features)\n",
        "        self.U_f = torch.nn.Linear(self.out_features, self.out_features, bias=False)\n",
        "\n",
        "    def forward(self, features, node_order, adjacency_list, edge_order):\n",
        "        '''Run TreeLSTM model on a tree data structure with node features\n",
        "        Takes Tensors encoding node features, a tree node adjacency_list, and the order in which \n",
        "        the tree processing should proceed in node_order and edge_order.\n",
        "        '''\n",
        "\n",
        "        # Total number of nodes in every tree in the batch\n",
        "        batch_size = node_order.shape[0]\n",
        "\n",
        "        # Retrive device the model is currently loaded on to generate h, c, and h_sum result buffers\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # h and c states for every node in the batch\n",
        "        h = torch.zeros(batch_size, self.out_features, device=device)\n",
        "        c = torch.zeros(batch_size, self.out_features, device=device)\n",
        "\n",
        "        # populate the h and c states respecting computation order\n",
        "        for n in range(node_order.max() + 1):\n",
        "            self._run_lstm(n, h, c, features, node_order, adjacency_list, edge_order)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "    def _run_lstm(self, iteration, h, c, features, node_order, adjacency_list, edge_order):\n",
        "        '''Helper function to evaluate all tree nodes currently able to be evaluated.\n",
        "        '''\n",
        "        # N is the number of nodes in the tree\n",
        "        # n is the number of nodes to be evaluated on in the current iteration\n",
        "        # E is the number of edges in the tree\n",
        "        # e is the number of edges to be evaluated on in the current iteration\n",
        "        # F is the number of features in each node\n",
        "        # M is the number of hidden neurons in the network\n",
        "\n",
        "        # node_order is a tensor of size N x 1\n",
        "        # edge_order is a tensor of size E x 1\n",
        "        # features is a tensor of size N x F\n",
        "        # adjacency_list is a tensor of size E x 2\n",
        "\n",
        "        # node_mask is a tensor of size N x 1\n",
        "        node_mask = node_order == iteration\n",
        "        # edge_mask is a tensor of size E x 1\n",
        "        edge_mask = edge_order == iteration\n",
        "\n",
        "        # x is a tensor of size n x F\n",
        "        x = features[node_mask, :]\n",
        "\n",
        "        # At iteration 0 none of the nodes should have children\n",
        "        # Otherwise, select the child nodes needed for current iteration\n",
        "        # and sum over their hidden states\n",
        "        if iteration == 0:\n",
        "            iou = self.W_iou(x)\n",
        "        else:\n",
        "            # adjacency_list is a tensor of size e x 2\n",
        "            adjacency_list = adjacency_list[edge_mask, :]\n",
        "\n",
        "            # parent_indexes and child_indexes are tensors of size e x 1\n",
        "            # parent_indexes and child_indexes contain the integer indexes needed to index into\n",
        "            # the feature and hidden state arrays to retrieve the data for those parent/child nodes.\n",
        "            parent_indexes = adjacency_list[:, 0]\n",
        "            child_indexes = adjacency_list[:, 1]\n",
        "\n",
        "            # child_h and child_c are tensors of size e x 1\n",
        "            child_h = h[child_indexes, :]\n",
        "            child_c = c[child_indexes, :]\n",
        "\n",
        "            # Add child hidden states to parent offset locations\n",
        "            _, child_counts = torch.unique_consecutive(parent_indexes, return_counts=True)\n",
        "            child_counts = tuple(child_counts)\n",
        "\n",
        "            parent_children = torch.split(child_h, child_counts)\n",
        "            parent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "            h_sum = torch.stack(parent_list)\n",
        "            iou = self.W_iou(x) + self.U_iou(h_sum)\n",
        "\n",
        "        # i, o and u are tensors of size n x M\n",
        "        i, o, u = torch.split(iou, iou.size(1) // 3, dim=1)\n",
        "        i = torch.sigmoid(i)\n",
        "        o = torch.sigmoid(o)\n",
        "        u = torch.tanh(u)\n",
        "\n",
        "        # At iteration 0 none of the nodes should have children\n",
        "        # Otherwise, calculate the forget states for each parent node and child node\n",
        "        # and sum over the child memory cell states\n",
        "        if iteration == 0:\n",
        "            c[node_mask, :] = i * u\n",
        "        else:\n",
        "            # f is a tensor of size e x M\n",
        "            f = self.W_f(features[parent_indexes, :]) + self.U_f(child_h)\n",
        "            f = torch.sigmoid(f)\n",
        "\n",
        "            # fc is a tensor of size e x M\n",
        "            fc = f * child_c\n",
        "\n",
        "            # Add the calculated f values to the parent's memory cell state\n",
        "            parent_children = torch.split(fc, child_counts)\n",
        "            parent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "            c_sum = torch.stack(parent_list)\n",
        "            c[node_mask, :] = i * u + c_sum\n",
        "\n",
        "        h[node_mask, :] = o * torch.tanh(c[node_mask])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pRS4Nocmvv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Helper functions for running the TreeLSTM model\n",
        "'''\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "\n",
        "def calculate_evaluation_orders(adjacency_list, tree_size):\n",
        "    '''Calculates the node_order and edge_order from a tree adjacency_list and the tree_size.\n",
        "    The TreeLSTM model requires node_order and edge_order to be passed into the model along\n",
        "    with the node features and adjacency_list.  We pre-calculate these orders as a speed\n",
        "    optimization.\n",
        "    '''\n",
        "    adjacency_list = numpy.array(adjacency_list)\n",
        "\n",
        "    node_ids = numpy.arange(tree_size, dtype=int)\n",
        "\n",
        "    node_order = numpy.zeros(tree_size, dtype=int)\n",
        "    unevaluated_nodes = numpy.ones(tree_size, dtype=bool)\n",
        "\n",
        "    parent_nodes = adjacency_list[:, 0]\n",
        "    child_nodes = adjacency_list[:, 1]\n",
        "\n",
        "    n = 0\n",
        "    while unevaluated_nodes.any():\n",
        "        # Find which child nodes have not been evaluated\n",
        "        unevaluated_mask = unevaluated_nodes[child_nodes]\n",
        "\n",
        "        # Find the parent nodes of unevaluated children\n",
        "        unready_parents = parent_nodes[unevaluated_mask]\n",
        "\n",
        "        # Mark nodes that have not yet been evaluated\n",
        "        # and which are not in the list of parents with unevaluated child nodes\n",
        "        nodes_to_evaluate = unevaluated_nodes & ~numpy.isin(node_ids, unready_parents)\n",
        "\n",
        "        node_order[nodes_to_evaluate] = n\n",
        "        unevaluated_nodes[nodes_to_evaluate] = False\n",
        "\n",
        "        n += 1\n",
        "\n",
        "    edge_order = node_order[parent_nodes]\n",
        "\n",
        "    return node_order, edge_order\n",
        "\n",
        "\n",
        "def batch_tree_input(batch):\n",
        "    '''Combines a batch of tree dictionaries into a single batched dictionary for use by the TreeLSTM model.\n",
        "    batch - list of dicts with keys ('features', 'node_order', 'edge_order', 'adjacency_list')\n",
        "    returns a dict with keys ('features', 'node_order', 'edge_order', 'adjacency_list', 'tree_sizes')\n",
        "    '''\n",
        "    tree_sizes = [b['features'].shape[0] for b in batch]\n",
        "\n",
        "    batched_features = torch.cat([b['features'] for b in batch])\n",
        "    batched_node_order = torch.cat([b['node_order'] for b in batch])\n",
        "    batched_edge_order = torch.cat([b['edge_order'] for b in batch])\n",
        "\n",
        "    batched_adjacency_list = []\n",
        "    offset = 0\n",
        "    for n, b in zip(tree_sizes, batch):\n",
        "        batched_adjacency_list.append(b['adjacency_list'] + offset)\n",
        "        offset += n\n",
        "    batched_adjacency_list = torch.cat(batched_adjacency_list)\n",
        "\n",
        "    return {\n",
        "        'features': batched_features,\n",
        "        'node_order': batched_node_order,\n",
        "        'edge_order': batched_edge_order,\n",
        "        'adjacency_list': batched_adjacency_list,\n",
        "        'tree_sizes': tree_sizes\n",
        "    }\n",
        "\n",
        "\n",
        "def unbatch_tree_tensor(tensor, tree_sizes):\n",
        "    '''Convenience functo to unbatch a batched tree tensor into individual tensors given an array of tree_sizes.\n",
        "    sum(tree_sizes) must equal the size of tensor's zeroth dimension.\n",
        "    '''\n",
        "    return torch.split(tensor, tree_sizes, dim=0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK4toD_Dm3_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eede3ee0-2549-43df-a3cd-d3ad5b324afd"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "def _label_node_index(node, n=0):\n",
        "    node['index'] = n\n",
        "    for child in node['children']:\n",
        "        n += 1\n",
        "        _label_node_index(child, n)\n",
        "\n",
        "\n",
        "def _gather_node_attributes(node, key):\n",
        "    features = [node[key]]\n",
        "    for child in node['children']:\n",
        "        features.extend(_gather_node_attributes(child, key))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _gather_adjacency_list(node):\n",
        "    adjacency_list = []\n",
        "    for child in node['children']:\n",
        "        adjacency_list.append([node['index'], child['index']])\n",
        "        adjacency_list.extend(_gather_adjacency_list(child))\n",
        "\n",
        "    return adjacency_list\n",
        "\n",
        "\n",
        "def convert_tree_to_tensors(tree, device=torch.device('cpu')):\n",
        "    # Label each node with its walk order to match nodes to feature tensor indexes\n",
        "    # This modifies the original tree as a side effect\n",
        "    _label_node_index(tree)\n",
        "\n",
        "    features = _gather_node_attributes(tree, 'features')\n",
        "    labels = _gather_node_attributes(tree, 'labels')\n",
        "    adjacency_list = _gather_adjacency_list(tree)\n",
        "\n",
        "    node_order, edge_order = calculate_evaluation_orders(adjacency_list, len(features))\n",
        "\n",
        "    return {\n",
        "        'features': torch.tensor(features, device=device, dtype=torch.float32),\n",
        "        'labels': torch.tensor(labels, device=device, dtype=torch.float32),\n",
        "        'node_order': torch.tensor(node_order, device=device, dtype=torch.int64),\n",
        "        'adjacency_list': torch.tensor(adjacency_list, device=device, dtype=torch.int64),\n",
        "        'edge_order': torch.tensor(edge_order, device=device, dtype=torch.int64),\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Toy example\n",
        "    tree = {\n",
        "        'features': [1, 0], 'labels': [1], 'children': [\n",
        "            {'features': [0, 1], 'labels': [0], 'children': []},\n",
        "            {'features': [0, 0], 'labels': [0], 'children': [\n",
        "                {'features': [1, 1], 'labels': [0], 'children': []}\n",
        "            ]},\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    data = convert_tree_to_tensors(tree)\n",
        "\n",
        "    model = TreeLSTM(2, 1).train()\n",
        "\n",
        "    loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    for n in range(1000):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        h, c = model(\n",
        "            data['features'],\n",
        "            data['node_order'],\n",
        "            data['adjacency_list'],\n",
        "            data['edge_order']\n",
        "        )\n",
        "\n",
        "        labels = data['labels']\n",
        "\n",
        "        loss = loss_function(h, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Iteration {n+1} Loss: {loss}')\n",
        "    print(data)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 Loss: 0.694767415523529\n",
            "Iteration 2 Loss: 0.6946672201156616\n",
            "Iteration 3 Loss: 0.6945676803588867\n",
            "Iteration 4 Loss: 0.6944689154624939\n",
            "Iteration 5 Loss: 0.6943709254264832\n",
            "Iteration 6 Loss: 0.6942735910415649\n",
            "Iteration 7 Loss: 0.6941771507263184\n",
            "Iteration 8 Loss: 0.6940813660621643\n",
            "Iteration 9 Loss: 0.6939864158630371\n",
            "Iteration 10 Loss: 0.693892240524292\n",
            "Iteration 11 Loss: 0.693798840045929\n",
            "Iteration 12 Loss: 0.6937063336372375\n",
            "Iteration 13 Loss: 0.6936144232749939\n",
            "Iteration 14 Loss: 0.6935235261917114\n",
            "Iteration 15 Loss: 0.6934332847595215\n",
            "Iteration 16 Loss: 0.6933438777923584\n",
            "Iteration 17 Loss: 0.6932551264762878\n",
            "Iteration 18 Loss: 0.6931673288345337\n",
            "Iteration 19 Loss: 0.6930801868438721\n",
            "Iteration 20 Loss: 0.6929938793182373\n",
            "Iteration 21 Loss: 0.6929082870483398\n",
            "Iteration 22 Loss: 0.6928233504295349\n",
            "Iteration 23 Loss: 0.6927392482757568\n",
            "Iteration 24 Loss: 0.6926558017730713\n",
            "Iteration 25 Loss: 0.6925731301307678\n",
            "Iteration 26 Loss: 0.6924910545349121\n",
            "Iteration 27 Loss: 0.6924098134040833\n",
            "Iteration 28 Loss: 0.6923292279243469\n",
            "Iteration 29 Loss: 0.6922492384910583\n",
            "Iteration 30 Loss: 0.6921699643135071\n",
            "Iteration 31 Loss: 0.6920914053916931\n",
            "Iteration 32 Loss: 0.6920133829116821\n",
            "Iteration 33 Loss: 0.6919360756874084\n",
            "Iteration 34 Loss: 0.6918593049049377\n",
            "Iteration 35 Loss: 0.6917831897735596\n",
            "Iteration 36 Loss: 0.6917077302932739\n",
            "Iteration 37 Loss: 0.6916327476501465\n",
            "Iteration 38 Loss: 0.6915584206581116\n",
            "Iteration 39 Loss: 0.6914845705032349\n",
            "Iteration 40 Loss: 0.6914113163948059\n",
            "Iteration 41 Loss: 0.6913385987281799\n",
            "Iteration 42 Loss: 0.6912662982940674\n",
            "Iteration 43 Loss: 0.6911945939064026\n",
            "Iteration 44 Loss: 0.6911233067512512\n",
            "Iteration 45 Loss: 0.6910524964332581\n",
            "Iteration 46 Loss: 0.6909822225570679\n",
            "Iteration 47 Loss: 0.6909122467041016\n",
            "Iteration 48 Loss: 0.6908427476882935\n",
            "Iteration 49 Loss: 0.6907736659049988\n",
            "Iteration 50 Loss: 0.6907049417495728\n",
            "Iteration 51 Loss: 0.6906366348266602\n",
            "Iteration 52 Loss: 0.6905686855316162\n",
            "Iteration 53 Loss: 0.6905010342597961\n",
            "Iteration 54 Loss: 0.6904337406158447\n",
            "Iteration 55 Loss: 0.6903667449951172\n",
            "Iteration 56 Loss: 0.6903001666069031\n",
            "Iteration 57 Loss: 0.6902337670326233\n",
            "Iteration 58 Loss: 0.6901676654815674\n",
            "Iteration 59 Loss: 0.6901018619537354\n",
            "Iteration 60 Loss: 0.6900362968444824\n",
            "Iteration 61 Loss: 0.6899709105491638\n",
            "Iteration 62 Loss: 0.6899058222770691\n",
            "Iteration 63 Loss: 0.6898409128189087\n",
            "Iteration 64 Loss: 0.6897762417793274\n",
            "Iteration 65 Loss: 0.6897118091583252\n",
            "Iteration 66 Loss: 0.6896474361419678\n",
            "Iteration 67 Loss: 0.6895832419395447\n",
            "Iteration 68 Loss: 0.6895192861557007\n",
            "Iteration 69 Loss: 0.689455509185791\n",
            "Iteration 70 Loss: 0.6893917918205261\n",
            "Iteration 71 Loss: 0.6893281936645508\n",
            "Iteration 72 Loss: 0.6892647743225098\n",
            "Iteration 73 Loss: 0.6892014741897583\n",
            "Iteration 74 Loss: 0.6891382932662964\n",
            "Iteration 75 Loss: 0.6890751719474792\n",
            "Iteration 76 Loss: 0.6890121698379517\n",
            "Iteration 77 Loss: 0.6889491677284241\n",
            "Iteration 78 Loss: 0.688886284828186\n",
            "Iteration 79 Loss: 0.6888235211372375\n",
            "Iteration 80 Loss: 0.6887606978416443\n",
            "Iteration 81 Loss: 0.6886979937553406\n",
            "Iteration 82 Loss: 0.6886352896690369\n",
            "Iteration 83 Loss: 0.6885725855827332\n",
            "Iteration 84 Loss: 0.688510000705719\n",
            "Iteration 85 Loss: 0.6884473562240601\n",
            "Iteration 86 Loss: 0.6883847713470459\n",
            "Iteration 87 Loss: 0.6883221864700317\n",
            "Iteration 88 Loss: 0.6882596015930176\n",
            "Iteration 89 Loss: 0.6881968975067139\n",
            "Iteration 90 Loss: 0.6881342530250549\n",
            "Iteration 91 Loss: 0.688071608543396\n",
            "Iteration 92 Loss: 0.6880089044570923\n",
            "Iteration 93 Loss: 0.6879461407661438\n",
            "Iteration 94 Loss: 0.6878833174705505\n",
            "Iteration 95 Loss: 0.6878204345703125\n",
            "Iteration 96 Loss: 0.6877575516700745\n",
            "Iteration 97 Loss: 0.6876944899559021\n",
            "Iteration 98 Loss: 0.6876314282417297\n",
            "Iteration 99 Loss: 0.6875683069229126\n",
            "Iteration 100 Loss: 0.6875050663948059\n",
            "Iteration 101 Loss: 0.6874417066574097\n",
            "Iteration 102 Loss: 0.6873781681060791\n",
            "Iteration 103 Loss: 0.6873146295547485\n",
            "Iteration 104 Loss: 0.6872509121894836\n",
            "Iteration 105 Loss: 0.6871870756149292\n",
            "Iteration 106 Loss: 0.6871231198310852\n",
            "Iteration 107 Loss: 0.6870591044425964\n",
            "Iteration 108 Loss: 0.6869947910308838\n",
            "Iteration 109 Loss: 0.6869304180145264\n",
            "Iteration 110 Loss: 0.6868659257888794\n",
            "Iteration 111 Loss: 0.6868011355400085\n",
            "Iteration 112 Loss: 0.6867362260818481\n",
            "Iteration 113 Loss: 0.6866711974143982\n",
            "Iteration 114 Loss: 0.6866059303283691\n",
            "Iteration 115 Loss: 0.6865404844284058\n",
            "Iteration 116 Loss: 0.6864748001098633\n",
            "Iteration 117 Loss: 0.6864089965820312\n",
            "Iteration 118 Loss: 0.6863429546356201\n",
            "Iteration 119 Loss: 0.6862766146659851\n",
            "Iteration 120 Loss: 0.6862101554870605\n",
            "Iteration 121 Loss: 0.6861433982849121\n",
            "Iteration 122 Loss: 0.6860764622688293\n",
            "Iteration 123 Loss: 0.6860092282295227\n",
            "Iteration 124 Loss: 0.685941755771637\n",
            "Iteration 125 Loss: 0.6858741044998169\n",
            "Iteration 126 Loss: 0.6858060956001282\n",
            "Iteration 127 Loss: 0.6857379078865051\n",
            "Iteration 128 Loss: 0.6856694221496582\n",
            "Iteration 129 Loss: 0.685600757598877\n",
            "Iteration 130 Loss: 0.6855316758155823\n",
            "Iteration 131 Loss: 0.6854622960090637\n",
            "Iteration 132 Loss: 0.6853927373886108\n",
            "Iteration 133 Loss: 0.6853227615356445\n",
            "Iteration 134 Loss: 0.6852526068687439\n",
            "Iteration 135 Loss: 0.6851820945739746\n",
            "Iteration 136 Loss: 0.6851112842559814\n",
            "Iteration 137 Loss: 0.6850401163101196\n",
            "Iteration 138 Loss: 0.6849685907363892\n",
            "Iteration 139 Loss: 0.6848968267440796\n",
            "Iteration 140 Loss: 0.6848247051239014\n",
            "Iteration 141 Loss: 0.6847522854804993\n",
            "Iteration 142 Loss: 0.6846795082092285\n",
            "Iteration 143 Loss: 0.6846063137054443\n",
            "Iteration 144 Loss: 0.6845328211784363\n",
            "Iteration 145 Loss: 0.6844589710235596\n",
            "Iteration 146 Loss: 0.6843847632408142\n",
            "Iteration 147 Loss: 0.6843101978302002\n",
            "Iteration 148 Loss: 0.6842352151870728\n",
            "Iteration 149 Loss: 0.6841598749160767\n",
            "Iteration 150 Loss: 0.6840842366218567\n",
            "Iteration 151 Loss: 0.6840081214904785\n",
            "Iteration 152 Loss: 0.6839317083358765\n",
            "Iteration 153 Loss: 0.6838548183441162\n",
            "Iteration 154 Loss: 0.6837775707244873\n",
            "Iteration 155 Loss: 0.6836999654769897\n",
            "Iteration 156 Loss: 0.6836218237876892\n",
            "Iteration 157 Loss: 0.6835434436798096\n",
            "Iteration 158 Loss: 0.683464527130127\n",
            "Iteration 159 Loss: 0.6833852529525757\n",
            "Iteration 160 Loss: 0.6833055019378662\n",
            "Iteration 161 Loss: 0.6832253932952881\n",
            "Iteration 162 Loss: 0.6831448674201965\n",
            "Iteration 163 Loss: 0.6830638647079468\n",
            "Iteration 164 Loss: 0.6829824447631836\n",
            "Iteration 165 Loss: 0.6829005479812622\n",
            "Iteration 166 Loss: 0.6828182935714722\n",
            "Iteration 167 Loss: 0.6827355027198792\n",
            "Iteration 168 Loss: 0.6826522350311279\n",
            "Iteration 169 Loss: 0.6825686693191528\n",
            "Iteration 170 Loss: 0.68248450756073\n",
            "Iteration 171 Loss: 0.6823999881744385\n",
            "Iteration 172 Loss: 0.6823148727416992\n",
            "Iteration 173 Loss: 0.6822293400764465\n",
            "Iteration 174 Loss: 0.6821433901786804\n",
            "Iteration 175 Loss: 0.6820569634437561\n",
            "Iteration 176 Loss: 0.6819700002670288\n",
            "Iteration 177 Loss: 0.6818826198577881\n",
            "Iteration 178 Loss: 0.6817947030067444\n",
            "Iteration 179 Loss: 0.6817062497138977\n",
            "Iteration 180 Loss: 0.6816173791885376\n",
            "Iteration 181 Loss: 0.6815279722213745\n",
            "Iteration 182 Loss: 0.681438148021698\n",
            "Iteration 183 Loss: 0.6813478469848633\n",
            "Iteration 184 Loss: 0.681256890296936\n",
            "Iteration 185 Loss: 0.6811654567718506\n",
            "Iteration 186 Loss: 0.6810735464096069\n",
            "Iteration 187 Loss: 0.6809811592102051\n",
            "Iteration 188 Loss: 0.6808882355690002\n",
            "Iteration 189 Loss: 0.6807947754859924\n",
            "Iteration 190 Loss: 0.6807007789611816\n",
            "Iteration 191 Loss: 0.6806062459945679\n",
            "Iteration 192 Loss: 0.6805112361907959\n",
            "Iteration 193 Loss: 0.680415689945221\n",
            "Iteration 194 Loss: 0.6803195476531982\n",
            "Iteration 195 Loss: 0.6802229881286621\n",
            "Iteration 196 Loss: 0.6801257729530334\n",
            "Iteration 197 Loss: 0.6800280213356018\n",
            "Iteration 198 Loss: 0.6799297332763672\n",
            "Iteration 199 Loss: 0.6798309683799744\n",
            "Iteration 200 Loss: 0.6797316074371338\n",
            "Iteration 201 Loss: 0.6796317100524902\n",
            "Iteration 202 Loss: 0.6795311570167542\n",
            "Iteration 203 Loss: 0.6794301271438599\n",
            "Iteration 204 Loss: 0.6793285012245178\n",
            "Iteration 205 Loss: 0.6792263984680176\n",
            "Iteration 206 Loss: 0.6791236400604248\n",
            "Iteration 207 Loss: 0.6790202856063843\n",
            "Iteration 208 Loss: 0.6789164543151855\n",
            "Iteration 209 Loss: 0.6788119673728943\n",
            "Iteration 210 Loss: 0.6787068843841553\n",
            "Iteration 211 Loss: 0.6786012649536133\n",
            "Iteration 212 Loss: 0.6784950494766235\n",
            "Iteration 213 Loss: 0.678388237953186\n",
            "Iteration 214 Loss: 0.6782808899879456\n",
            "Iteration 215 Loss: 0.6781728863716125\n",
            "Iteration 216 Loss: 0.6780642867088318\n",
            "Iteration 217 Loss: 0.6779550909996033\n",
            "Iteration 218 Loss: 0.6778453588485718\n",
            "Iteration 219 Loss: 0.677734911441803\n",
            "Iteration 220 Loss: 0.677623987197876\n",
            "Iteration 221 Loss: 0.6775124073028564\n",
            "Iteration 222 Loss: 0.6774001717567444\n",
            "Iteration 223 Loss: 0.6772872805595398\n",
            "Iteration 224 Loss: 0.6771738529205322\n",
            "Iteration 225 Loss: 0.6770598888397217\n",
            "Iteration 226 Loss: 0.6769452095031738\n",
            "Iteration 227 Loss: 0.6768298149108887\n",
            "Iteration 228 Loss: 0.6767138838768005\n",
            "Iteration 229 Loss: 0.6765973567962646\n",
            "Iteration 230 Loss: 0.6764801740646362\n",
            "Iteration 231 Loss: 0.6763622760772705\n",
            "Iteration 232 Loss: 0.6762438416481018\n",
            "Iteration 233 Loss: 0.6761248111724854\n",
            "Iteration 234 Loss: 0.6760050058364868\n",
            "Iteration 235 Loss: 0.6758846640586853\n",
            "Iteration 236 Loss: 0.6757636070251465\n",
            "Iteration 237 Loss: 0.6756419539451599\n",
            "Iteration 238 Loss: 0.675519585609436\n",
            "Iteration 239 Loss: 0.6753966212272644\n",
            "Iteration 240 Loss: 0.6752729415893555\n",
            "Iteration 241 Loss: 0.6751486659049988\n",
            "Iteration 242 Loss: 0.6750236749649048\n",
            "Iteration 243 Loss: 0.6748980283737183\n",
            "Iteration 244 Loss: 0.674771785736084\n",
            "Iteration 245 Loss: 0.6746448278427124\n",
            "Iteration 246 Loss: 0.6745171546936035\n",
            "Iteration 247 Loss: 0.6743888854980469\n",
            "Iteration 248 Loss: 0.6742599010467529\n",
            "Iteration 249 Loss: 0.6741302013397217\n",
            "Iteration 250 Loss: 0.6739999055862427\n",
            "Iteration 251 Loss: 0.6738688349723816\n",
            "Iteration 252 Loss: 0.673737108707428\n",
            "Iteration 253 Loss: 0.6736047267913818\n",
            "Iteration 254 Loss: 0.6734716296195984\n",
            "Iteration 255 Loss: 0.6733378171920776\n",
            "Iteration 256 Loss: 0.6732033491134644\n",
            "Iteration 257 Loss: 0.673068106174469\n",
            "Iteration 258 Loss: 0.6729322671890259\n",
            "Iteration 259 Loss: 0.6727957129478455\n",
            "Iteration 260 Loss: 0.6726583242416382\n",
            "Iteration 261 Loss: 0.6725203394889832\n",
            "Iteration 262 Loss: 0.6723816394805908\n",
            "Iteration 263 Loss: 0.6722421646118164\n",
            "Iteration 264 Loss: 0.6721020936965942\n",
            "Iteration 265 Loss: 0.6719611883163452\n",
            "Iteration 266 Loss: 0.6718195676803589\n",
            "Iteration 267 Loss: 0.6716772317886353\n",
            "Iteration 268 Loss: 0.6715342402458191\n",
            "Iteration 269 Loss: 0.6713904738426208\n",
            "Iteration 270 Loss: 0.6712459325790405\n",
            "Iteration 271 Loss: 0.6711007356643677\n",
            "Iteration 272 Loss: 0.670954704284668\n",
            "Iteration 273 Loss: 0.670807957649231\n",
            "Iteration 274 Loss: 0.6706605553627014\n",
            "Iteration 275 Loss: 0.670512318611145\n",
            "Iteration 276 Loss: 0.6703633666038513\n",
            "Iteration 277 Loss: 0.6702136993408203\n",
            "Iteration 278 Loss: 0.6700632572174072\n",
            "Iteration 279 Loss: 0.6699120402336121\n",
            "Iteration 280 Loss: 0.6697600483894348\n",
            "Iteration 281 Loss: 0.6696073412895203\n",
            "Iteration 282 Loss: 0.6694537401199341\n",
            "Iteration 283 Loss: 0.6692995429039001\n",
            "Iteration 284 Loss: 0.6691445112228394\n",
            "Iteration 285 Loss: 0.6689887046813965\n",
            "Iteration 286 Loss: 0.6688320636749268\n",
            "Iteration 287 Loss: 0.6686747670173645\n",
            "Iteration 288 Loss: 0.6685166358947754\n",
            "Iteration 289 Loss: 0.6683577299118042\n",
            "Iteration 290 Loss: 0.6681979894638062\n",
            "Iteration 291 Loss: 0.6680375337600708\n",
            "Iteration 292 Loss: 0.6678761839866638\n",
            "Iteration 293 Loss: 0.6677140593528748\n",
            "Iteration 294 Loss: 0.6675511598587036\n",
            "Iteration 295 Loss: 0.6673875451087952\n",
            "Iteration 296 Loss: 0.6672229766845703\n",
            "Iteration 297 Loss: 0.6670576930046082\n",
            "Iteration 298 Loss: 0.6668916344642639\n",
            "Iteration 299 Loss: 0.666724681854248\n",
            "Iteration 300 Loss: 0.6665569543838501\n",
            "Iteration 301 Loss: 0.6663883924484253\n",
            "Iteration 302 Loss: 0.6662189960479736\n",
            "Iteration 303 Loss: 0.6660488247871399\n",
            "Iteration 304 Loss: 0.6658777594566345\n",
            "Iteration 305 Loss: 0.6657059192657471\n",
            "Iteration 306 Loss: 0.665533185005188\n",
            "Iteration 307 Loss: 0.665359616279602\n",
            "Iteration 308 Loss: 0.665185272693634\n",
            "Iteration 309 Loss: 0.6650100350379944\n",
            "Iteration 310 Loss: 0.6648340225219727\n",
            "Iteration 311 Loss: 0.6646571159362793\n",
            "Iteration 312 Loss: 0.6644793152809143\n",
            "Iteration 313 Loss: 0.6643006801605225\n",
            "Iteration 314 Loss: 0.664121150970459\n",
            "Iteration 315 Loss: 0.6639408469200134\n",
            "Iteration 316 Loss: 0.6637596487998962\n",
            "Iteration 317 Loss: 0.6635775566101074\n",
            "Iteration 318 Loss: 0.6633945107460022\n",
            "Iteration 319 Loss: 0.6632106900215149\n",
            "Iteration 320 Loss: 0.663025975227356\n",
            "Iteration 321 Loss: 0.6628403663635254\n",
            "Iteration 322 Loss: 0.662653923034668\n",
            "Iteration 323 Loss: 0.6624665260314941\n",
            "Iteration 324 Loss: 0.6622782349586487\n",
            "Iteration 325 Loss: 0.6620890498161316\n",
            "Iteration 326 Loss: 0.6618989706039429\n",
            "Iteration 327 Loss: 0.6617079973220825\n",
            "Iteration 328 Loss: 0.6615160703659058\n",
            "Iteration 329 Loss: 0.6613232493400574\n",
            "Iteration 330 Loss: 0.6611295342445374\n",
            "Iteration 331 Loss: 0.6609349250793457\n",
            "Iteration 332 Loss: 0.6607393622398376\n",
            "Iteration 333 Loss: 0.6605428457260132\n",
            "Iteration 334 Loss: 0.6603454351425171\n",
            "Iteration 335 Loss: 0.6601470708847046\n",
            "Iteration 336 Loss: 0.6599477529525757\n",
            "Iteration 337 Loss: 0.6597474813461304\n",
            "Iteration 338 Loss: 0.6595462560653687\n",
            "Iteration 339 Loss: 0.6593441963195801\n",
            "Iteration 340 Loss: 0.6591411232948303\n",
            "Iteration 341 Loss: 0.6589369773864746\n",
            "Iteration 342 Loss: 0.6587319374084473\n",
            "Iteration 343 Loss: 0.6585259437561035\n",
            "Iteration 344 Loss: 0.6583190560340881\n",
            "Iteration 345 Loss: 0.658111035823822\n",
            "Iteration 346 Loss: 0.657902181148529\n",
            "Iteration 347 Loss: 0.6576922535896301\n",
            "Iteration 348 Loss: 0.65748131275177\n",
            "Iteration 349 Loss: 0.6572694778442383\n",
            "Iteration 350 Loss: 0.6570565104484558\n",
            "Iteration 351 Loss: 0.6568426489830017\n",
            "Iteration 352 Loss: 0.6566277742385864\n",
            "Iteration 353 Loss: 0.6564118266105652\n",
            "Iteration 354 Loss: 0.6561949253082275\n",
            "Iteration 355 Loss: 0.6559770107269287\n",
            "Iteration 356 Loss: 0.6557580232620239\n",
            "Iteration 357 Loss: 0.6555379629135132\n",
            "Iteration 358 Loss: 0.655316948890686\n",
            "Iteration 359 Loss: 0.6550948619842529\n",
            "Iteration 360 Loss: 0.6548717021942139\n",
            "Iteration 361 Loss: 0.6546475291252136\n",
            "Iteration 362 Loss: 0.6544222831726074\n",
            "Iteration 363 Loss: 0.65419602394104\n",
            "Iteration 364 Loss: 0.6539686322212219\n",
            "Iteration 365 Loss: 0.6537401676177979\n",
            "Iteration 366 Loss: 0.6535106897354126\n",
            "Iteration 367 Loss: 0.6532801389694214\n",
            "Iteration 368 Loss: 0.6530483961105347\n",
            "Iteration 369 Loss: 0.652815580368042\n",
            "Iteration 370 Loss: 0.6525817513465881\n",
            "Iteration 371 Loss: 0.6523467898368835\n",
            "Iteration 372 Loss: 0.6521106958389282\n",
            "Iteration 373 Loss: 0.6518734693527222\n",
            "Iteration 374 Loss: 0.6516352295875549\n",
            "Iteration 375 Loss: 0.6513956785202026\n",
            "Iteration 376 Loss: 0.6511551141738892\n",
            "Iteration 377 Loss: 0.650913417339325\n",
            "Iteration 378 Loss: 0.6506705284118652\n",
            "Iteration 379 Loss: 0.6504265666007996\n",
            "Iteration 380 Loss: 0.6501814126968384\n",
            "Iteration 381 Loss: 0.6499350666999817\n",
            "Iteration 382 Loss: 0.6496875286102295\n",
            "Iteration 383 Loss: 0.6494389176368713\n",
            "Iteration 384 Loss: 0.6491889953613281\n",
            "Iteration 385 Loss: 0.6489379405975342\n",
            "Iteration 386 Loss: 0.6486857533454895\n",
            "Iteration 387 Loss: 0.6484323143959045\n",
            "Iteration 388 Loss: 0.6481776833534241\n",
            "Iteration 389 Loss: 0.6479218602180481\n",
            "Iteration 390 Loss: 0.6476647853851318\n",
            "Iteration 391 Loss: 0.6474064588546753\n",
            "Iteration 392 Loss: 0.6471469402313232\n",
            "Iteration 393 Loss: 0.6468861103057861\n",
            "Iteration 394 Loss: 0.6466240882873535\n",
            "Iteration 395 Loss: 0.6463608145713806\n",
            "Iteration 396 Loss: 0.6460962295532227\n",
            "Iteration 397 Loss: 0.6458303928375244\n",
            "Iteration 398 Loss: 0.6455633640289307\n",
            "Iteration 399 Loss: 0.6452949047088623\n",
            "Iteration 400 Loss: 0.6450252532958984\n",
            "Iteration 401 Loss: 0.6447542309761047\n",
            "Iteration 402 Loss: 0.6444820165634155\n",
            "Iteration 403 Loss: 0.6442083716392517\n",
            "Iteration 404 Loss: 0.6439334750175476\n",
            "Iteration 405 Loss: 0.6436572074890137\n",
            "Iteration 406 Loss: 0.6433796882629395\n",
            "Iteration 407 Loss: 0.6431006789207458\n",
            "Iteration 408 Loss: 0.6428203582763672\n",
            "Iteration 409 Loss: 0.6425386667251587\n",
            "Iteration 410 Loss: 0.6422556042671204\n",
            "Iteration 411 Loss: 0.641971230506897\n",
            "Iteration 412 Loss: 0.6416853666305542\n",
            "Iteration 413 Loss: 0.6413981914520264\n",
            "Iteration 414 Loss: 0.6411095857620239\n",
            "Iteration 415 Loss: 0.6408195495605469\n",
            "Iteration 416 Loss: 0.6405280828475952\n",
            "Iteration 417 Loss: 0.6402351260185242\n",
            "Iteration 418 Loss: 0.6399407982826233\n",
            "Iteration 419 Loss: 0.6396450400352478\n",
            "Iteration 420 Loss: 0.6393477320671082\n",
            "Iteration 421 Loss: 0.6390490531921387\n",
            "Iteration 422 Loss: 0.6387487649917603\n",
            "Iteration 423 Loss: 0.638447105884552\n",
            "Iteration 424 Loss: 0.6381439566612244\n",
            "Iteration 425 Loss: 0.6378391981124878\n",
            "Iteration 426 Loss: 0.6375329494476318\n",
            "Iteration 427 Loss: 0.6372252106666565\n",
            "Iteration 428 Loss: 0.636915922164917\n",
            "Iteration 429 Loss: 0.6366050839424133\n",
            "Iteration 430 Loss: 0.636292576789856\n",
            "Iteration 431 Loss: 0.6359786987304688\n",
            "Iteration 432 Loss: 0.6356631517410278\n",
            "Iteration 433 Loss: 0.6353459358215332\n",
            "Iteration 434 Loss: 0.6350272297859192\n",
            "Iteration 435 Loss: 0.6347068548202515\n",
            "Iteration 436 Loss: 0.6343848705291748\n",
            "Iteration 437 Loss: 0.6340612769126892\n",
            "Iteration 438 Loss: 0.6337360143661499\n",
            "Iteration 439 Loss: 0.6334091424942017\n",
            "Iteration 440 Loss: 0.6330805420875549\n",
            "Iteration 441 Loss: 0.6327502727508545\n",
            "Iteration 442 Loss: 0.6324183940887451\n",
            "Iteration 443 Loss: 0.6320847868919373\n",
            "Iteration 444 Loss: 0.6317495107650757\n",
            "Iteration 445 Loss: 0.6314123868942261\n",
            "Iteration 446 Loss: 0.6310737133026123\n",
            "Iteration 447 Loss: 0.6307331919670105\n",
            "Iteration 448 Loss: 0.6303909420967102\n",
            "Iteration 449 Loss: 0.6300469040870667\n",
            "Iteration 450 Loss: 0.6297011375427246\n",
            "Iteration 451 Loss: 0.6293535828590393\n",
            "Iteration 452 Loss: 0.629004180431366\n",
            "Iteration 453 Loss: 0.6286530494689941\n",
            "Iteration 454 Loss: 0.6283000707626343\n",
            "Iteration 455 Loss: 0.6279453039169312\n",
            "Iteration 456 Loss: 0.6275886297225952\n",
            "Iteration 457 Loss: 0.627230167388916\n",
            "Iteration 458 Loss: 0.626869797706604\n",
            "Iteration 459 Loss: 0.6265076994895935\n",
            "Iteration 460 Loss: 0.6261435747146606\n",
            "Iteration 461 Loss: 0.6257776021957397\n",
            "Iteration 462 Loss: 0.6254098415374756\n",
            "Iteration 463 Loss: 0.6250399947166443\n",
            "Iteration 464 Loss: 0.624668300151825\n",
            "Iteration 465 Loss: 0.6242947578430176\n",
            "Iteration 466 Loss: 0.6239191293716431\n",
            "Iteration 467 Loss: 0.6235416531562805\n",
            "Iteration 468 Loss: 0.6231621503829956\n",
            "Iteration 469 Loss: 0.6227807402610779\n",
            "Iteration 470 Loss: 0.6223973631858826\n",
            "Iteration 471 Loss: 0.6220119595527649\n",
            "Iteration 472 Loss: 0.6216245889663696\n",
            "Iteration 473 Loss: 0.621235191822052\n",
            "Iteration 474 Loss: 0.620843768119812\n",
            "Iteration 475 Loss: 0.6204503774642944\n",
            "Iteration 476 Loss: 0.6200549602508545\n",
            "Iteration 477 Loss: 0.6196574568748474\n",
            "Iteration 478 Loss: 0.619257926940918\n",
            "Iteration 479 Loss: 0.6188563704490662\n",
            "Iteration 480 Loss: 0.618452787399292\n",
            "Iteration 481 Loss: 0.6180469989776611\n",
            "Iteration 482 Loss: 0.6176392436027527\n",
            "Iteration 483 Loss: 0.6172294616699219\n",
            "Iteration 484 Loss: 0.6168174743652344\n",
            "Iteration 485 Loss: 0.6164034605026245\n",
            "Iteration 486 Loss: 0.6159873604774475\n",
            "Iteration 487 Loss: 0.6155691742897034\n",
            "Iteration 488 Loss: 0.6151488423347473\n",
            "Iteration 489 Loss: 0.6147264838218689\n",
            "Iteration 490 Loss: 0.6143019199371338\n",
            "Iteration 491 Loss: 0.6138753294944763\n",
            "Iteration 492 Loss: 0.6134465932846069\n",
            "Iteration 493 Loss: 0.6130157113075256\n",
            "Iteration 494 Loss: 0.6125827431678772\n",
            "Iteration 495 Loss: 0.6121476888656616\n",
            "Iteration 496 Loss: 0.6117104291915894\n",
            "Iteration 497 Loss: 0.61127108335495\n",
            "Iteration 498 Loss: 0.6108295917510986\n",
            "Iteration 499 Loss: 0.6103861331939697\n",
            "Iteration 500 Loss: 0.6099404096603394\n",
            "Iteration 501 Loss: 0.6094925999641418\n",
            "Iteration 502 Loss: 0.609042763710022\n",
            "Iteration 503 Loss: 0.6085906624794006\n",
            "Iteration 504 Loss: 0.6081365942955017\n",
            "Iteration 505 Loss: 0.6076804399490356\n",
            "Iteration 506 Loss: 0.6072221398353577\n",
            "Iteration 507 Loss: 0.6067617535591125\n",
            "Iteration 508 Loss: 0.6062992811203003\n",
            "Iteration 509 Loss: 0.6058347225189209\n",
            "Iteration 510 Loss: 0.6053681373596191\n",
            "Iteration 511 Loss: 0.604899525642395\n",
            "Iteration 512 Loss: 0.6044288277626038\n",
            "Iteration 513 Loss: 0.6039561033248901\n",
            "Iteration 514 Loss: 0.6034813523292542\n",
            "Iteration 515 Loss: 0.6030045747756958\n",
            "Iteration 516 Loss: 0.6025257706642151\n",
            "Iteration 517 Loss: 0.6020450592041016\n",
            "Iteration 518 Loss: 0.6015623211860657\n",
            "Iteration 519 Loss: 0.6010776162147522\n",
            "Iteration 520 Loss: 0.6005909442901611\n",
            "Iteration 521 Loss: 0.6001023650169373\n",
            "Iteration 522 Loss: 0.5996119379997253\n",
            "Iteration 523 Loss: 0.5991195440292358\n",
            "Iteration 524 Loss: 0.5986252427101135\n",
            "Iteration 525 Loss: 0.598129153251648\n",
            "Iteration 526 Loss: 0.5976312160491943\n",
            "Iteration 527 Loss: 0.5971314311027527\n",
            "Iteration 528 Loss: 0.596629798412323\n",
            "Iteration 529 Loss: 0.5961264371871948\n",
            "Iteration 530 Loss: 0.5956213474273682\n",
            "Iteration 531 Loss: 0.595114529132843\n",
            "Iteration 532 Loss: 0.5946059823036194\n",
            "Iteration 533 Loss: 0.5940957069396973\n",
            "Iteration 534 Loss: 0.5935837626457214\n",
            "Iteration 535 Loss: 0.5930702686309814\n",
            "Iteration 536 Loss: 0.5925551056861877\n",
            "Iteration 537 Loss: 0.5920383334159851\n",
            "Iteration 538 Loss: 0.5915200710296631\n",
            "Iteration 539 Loss: 0.5910002589225769\n",
            "Iteration 540 Loss: 0.5904788970947266\n",
            "Iteration 541 Loss: 0.5899561643600464\n",
            "Iteration 542 Loss: 0.589431881904602\n",
            "Iteration 543 Loss: 0.5889062881469727\n",
            "Iteration 544 Loss: 0.5883792638778687\n",
            "Iteration 545 Loss: 0.5878509283065796\n",
            "Iteration 546 Loss: 0.5873212814331055\n",
            "Iteration 547 Loss: 0.5867902636528015\n",
            "Iteration 548 Loss: 0.5862581133842468\n",
            "Iteration 549 Loss: 0.5857247114181519\n",
            "Iteration 550 Loss: 0.5851900577545166\n",
            "Iteration 551 Loss: 0.5846543908119202\n",
            "Iteration 552 Loss: 0.5841175317764282\n",
            "Iteration 553 Loss: 0.5835795998573303\n",
            "Iteration 554 Loss: 0.5830406546592712\n",
            "Iteration 555 Loss: 0.582500696182251\n",
            "Iteration 556 Loss: 0.5819597840309143\n",
            "Iteration 557 Loss: 0.5814179182052612\n",
            "Iteration 558 Loss: 0.5808751583099365\n",
            "Iteration 559 Loss: 0.5803316235542297\n",
            "Iteration 560 Loss: 0.5797871351242065\n",
            "Iteration 561 Loss: 0.5792419910430908\n",
            "Iteration 562 Loss: 0.578696072101593\n",
            "Iteration 563 Loss: 0.5781494379043579\n",
            "Iteration 564 Loss: 0.5776020884513855\n",
            "Iteration 565 Loss: 0.5770542025566101\n",
            "Iteration 566 Loss: 0.576505720615387\n",
            "Iteration 567 Loss: 0.5759566426277161\n",
            "Iteration 568 Loss: 0.575407087802887\n",
            "Iteration 569 Loss: 0.5748570561408997\n",
            "Iteration 570 Loss: 0.5743066072463989\n",
            "Iteration 571 Loss: 0.5737557411193848\n",
            "Iteration 572 Loss: 0.5732045769691467\n",
            "Iteration 573 Loss: 0.57265305519104\n",
            "Iteration 574 Loss: 0.5721012353897095\n",
            "Iteration 575 Loss: 0.5715492367744446\n",
            "Iteration 576 Loss: 0.5709970593452454\n",
            "Iteration 577 Loss: 0.5704447031021118\n",
            "Iteration 578 Loss: 0.5698922276496887\n",
            "Iteration 579 Loss: 0.5693396925926208\n",
            "Iteration 580 Loss: 0.5687870979309082\n",
            "Iteration 581 Loss: 0.5682345032691956\n",
            "Iteration 582 Loss: 0.5676819682121277\n",
            "Iteration 583 Loss: 0.5671294927597046\n",
            "Iteration 584 Loss: 0.5665771961212158\n",
            "Iteration 585 Loss: 0.5660249590873718\n",
            "Iteration 586 Loss: 0.5654729604721069\n",
            "Iteration 587 Loss: 0.5649212002754211\n",
            "Iteration 588 Loss: 0.5643696188926697\n",
            "Iteration 589 Loss: 0.5638184547424316\n",
            "Iteration 590 Loss: 0.5632675886154175\n",
            "Iteration 591 Loss: 0.562717080116272\n",
            "Iteration 592 Loss: 0.5621669888496399\n",
            "Iteration 593 Loss: 0.5616174340248108\n",
            "Iteration 594 Loss: 0.5610682368278503\n",
            "Iteration 595 Loss: 0.5605195760726929\n",
            "Iteration 596 Loss: 0.5599715709686279\n",
            "Iteration 597 Loss: 0.5594240427017212\n",
            "Iteration 598 Loss: 0.5588772296905518\n",
            "Iteration 599 Loss: 0.5583309531211853\n",
            "Iteration 600 Loss: 0.5577853918075562\n",
            "Iteration 601 Loss: 0.5572406053543091\n",
            "Iteration 602 Loss: 0.5566965341567993\n",
            "Iteration 603 Loss: 0.5561532974243164\n",
            "Iteration 604 Loss: 0.5556108355522156\n",
            "Iteration 605 Loss: 0.5550692081451416\n",
            "Iteration 606 Loss: 0.5545284152030945\n",
            "Iteration 607 Loss: 0.5539885759353638\n",
            "Iteration 608 Loss: 0.5534496307373047\n",
            "Iteration 609 Loss: 0.5529117584228516\n",
            "Iteration 610 Loss: 0.5523747801780701\n",
            "Iteration 611 Loss: 0.5518388748168945\n",
            "Iteration 612 Loss: 0.5513039827346802\n",
            "Iteration 613 Loss: 0.5507701635360718\n",
            "Iteration 614 Loss: 0.5502374172210693\n",
            "Iteration 615 Loss: 0.5497059226036072\n",
            "Iteration 616 Loss: 0.549175500869751\n",
            "Iteration 617 Loss: 0.5486462116241455\n",
            "Iteration 618 Loss: 0.5481182932853699\n",
            "Iteration 619 Loss: 0.5475914478302002\n",
            "Iteration 620 Loss: 0.5470658540725708\n",
            "Iteration 621 Loss: 0.5465415716171265\n",
            "Iteration 622 Loss: 0.5460186004638672\n",
            "Iteration 623 Loss: 0.5454970002174377\n",
            "Iteration 624 Loss: 0.5449766516685486\n",
            "Iteration 625 Loss: 0.5444576740264893\n",
            "Iteration 626 Loss: 0.5439401865005493\n",
            "Iteration 627 Loss: 0.5434240102767944\n",
            "Iteration 628 Loss: 0.5429093241691589\n",
            "Iteration 629 Loss: 0.5423960089683533\n",
            "Iteration 630 Loss: 0.541884183883667\n",
            "Iteration 631 Loss: 0.5413738489151001\n",
            "Iteration 632 Loss: 0.5408649444580078\n",
            "Iteration 633 Loss: 0.5403577089309692\n",
            "Iteration 634 Loss: 0.5398519039154053\n",
            "Iteration 635 Loss: 0.5393477082252502\n",
            "Iteration 636 Loss: 0.5388450622558594\n",
            "Iteration 637 Loss: 0.5383440256118774\n",
            "Iteration 638 Loss: 0.5378444790840149\n",
            "Iteration 639 Loss: 0.537346601486206\n",
            "Iteration 640 Loss: 0.5368503928184509\n",
            "Iteration 641 Loss: 0.5363557934761047\n",
            "Iteration 642 Loss: 0.535862922668457\n",
            "Iteration 643 Loss: 0.5353716015815735\n",
            "Iteration 644 Loss: 0.5348820090293884\n",
            "Iteration 645 Loss: 0.5343941450119019\n",
            "Iteration 646 Loss: 0.5339078903198242\n",
            "Iteration 647 Loss: 0.5334234833717346\n",
            "Iteration 648 Loss: 0.5329407453536987\n",
            "Iteration 649 Loss: 0.5324596762657166\n",
            "Iteration 650 Loss: 0.5319804549217224\n",
            "Iteration 651 Loss: 0.5315029621124268\n",
            "Iteration 652 Loss: 0.5310271978378296\n",
            "Iteration 653 Loss: 0.5305532217025757\n",
            "Iteration 654 Loss: 0.5300810933113098\n",
            "Iteration 655 Loss: 0.5296106338500977\n",
            "Iteration 656 Loss: 0.5291420221328735\n",
            "Iteration 657 Loss: 0.5286751985549927\n",
            "Iteration 658 Loss: 0.5282102823257446\n",
            "Iteration 659 Loss: 0.5277471542358398\n",
            "Iteration 660 Loss: 0.5272858142852783\n",
            "Iteration 661 Loss: 0.5268262624740601\n",
            "Iteration 662 Loss: 0.5263685584068298\n",
            "Iteration 663 Loss: 0.5259127616882324\n",
            "Iteration 664 Loss: 0.5254587531089783\n",
            "Iteration 665 Loss: 0.5250065326690674\n",
            "Iteration 666 Loss: 0.5245562791824341\n",
            "Iteration 667 Loss: 0.524107813835144\n",
            "Iteration 668 Loss: 0.523661196231842\n",
            "Iteration 669 Loss: 0.5232166051864624\n",
            "Iteration 670 Loss: 0.5227736830711365\n",
            "Iteration 671 Loss: 0.5223326683044434\n",
            "Iteration 672 Loss: 0.5218935012817383\n",
            "Iteration 673 Loss: 0.5214563012123108\n",
            "Iteration 674 Loss: 0.5210208892822266\n",
            "Iteration 675 Loss: 0.5205874443054199\n",
            "Iteration 676 Loss: 0.5201557874679565\n",
            "Iteration 677 Loss: 0.519726037979126\n",
            "Iteration 678 Loss: 0.5192980766296387\n",
            "Iteration 679 Loss: 0.518872082233429\n",
            "Iteration 680 Loss: 0.5184479355812073\n",
            "Iteration 681 Loss: 0.5180256962776184\n",
            "Iteration 682 Loss: 0.5176052451133728\n",
            "Iteration 683 Loss: 0.5171867609024048\n",
            "Iteration 684 Loss: 0.5167701244354248\n",
            "Iteration 685 Loss: 0.5163553357124329\n",
            "Iteration 686 Loss: 0.5159424543380737\n",
            "Iteration 687 Loss: 0.5155314207077026\n",
            "Iteration 688 Loss: 0.5151221752166748\n",
            "Iteration 689 Loss: 0.5147148370742798\n",
            "Iteration 690 Loss: 0.5143094062805176\n",
            "Iteration 691 Loss: 0.5139057636260986\n",
            "Iteration 692 Loss: 0.5135040283203125\n",
            "Iteration 693 Loss: 0.5131041407585144\n",
            "Iteration 694 Loss: 0.5127060413360596\n",
            "Iteration 695 Loss: 0.5123098492622375\n",
            "Iteration 696 Loss: 0.5119154453277588\n",
            "Iteration 697 Loss: 0.5115228891372681\n",
            "Iteration 698 Loss: 0.5111321806907654\n",
            "Iteration 699 Loss: 0.510743260383606\n",
            "Iteration 700 Loss: 0.5103561878204346\n",
            "Iteration 701 Loss: 0.509971022605896\n",
            "Iteration 702 Loss: 0.5095875859260559\n",
            "Iteration 703 Loss: 0.5092059373855591\n",
            "Iteration 704 Loss: 0.5088260769844055\n",
            "Iteration 705 Loss: 0.50844806432724\n",
            "Iteration 706 Loss: 0.5080718994140625\n",
            "Iteration 707 Loss: 0.5076974630355835\n",
            "Iteration 708 Loss: 0.5073248147964478\n",
            "Iteration 709 Loss: 0.5069538354873657\n",
            "Iteration 710 Loss: 0.5065847635269165\n",
            "Iteration 711 Loss: 0.5062174201011658\n",
            "Iteration 712 Loss: 0.5058518648147583\n",
            "Iteration 713 Loss: 0.5054880380630493\n",
            "Iteration 714 Loss: 0.5051259994506836\n",
            "Iteration 715 Loss: 0.5047656297683716\n",
            "Iteration 716 Loss: 0.5044070482254028\n",
            "Iteration 717 Loss: 0.5040501356124878\n",
            "Iteration 718 Loss: 0.503695011138916\n",
            "Iteration 719 Loss: 0.503341555595398\n",
            "Iteration 720 Loss: 0.5029898285865784\n",
            "Iteration 721 Loss: 0.5026398301124573\n",
            "Iteration 722 Loss: 0.5022915601730347\n",
            "Iteration 723 Loss: 0.501944899559021\n",
            "Iteration 724 Loss: 0.501599907875061\n",
            "Iteration 725 Loss: 0.5012566447257996\n",
            "Iteration 726 Loss: 0.5009150505065918\n",
            "Iteration 727 Loss: 0.500575065612793\n",
            "Iteration 728 Loss: 0.5002367496490479\n",
            "Iteration 729 Loss: 0.49990004301071167\n",
            "Iteration 730 Loss: 0.499565064907074\n",
            "Iteration 731 Loss: 0.4992317259311676\n",
            "Iteration 732 Loss: 0.4988999366760254\n",
            "Iteration 733 Loss: 0.4985698461532593\n",
            "Iteration 734 Loss: 0.49824124574661255\n",
            "Iteration 735 Loss: 0.4979143440723419\n",
            "Iteration 736 Loss: 0.49758899211883545\n",
            "Iteration 737 Loss: 0.4972652196884155\n",
            "Iteration 738 Loss: 0.49694308638572693\n",
            "Iteration 739 Loss: 0.4966224730014801\n",
            "Iteration 740 Loss: 0.49630340933799744\n",
            "Iteration 741 Loss: 0.4959859848022461\n",
            "Iteration 742 Loss: 0.49567002058029175\n",
            "Iteration 743 Loss: 0.49535563588142395\n",
            "Iteration 744 Loss: 0.4950428009033203\n",
            "Iteration 745 Loss: 0.49473142623901367\n",
            "Iteration 746 Loss: 0.49442166090011597\n",
            "Iteration 747 Loss: 0.49411332607269287\n",
            "Iteration 748 Loss: 0.49380654096603394\n",
            "Iteration 749 Loss: 0.493501216173172\n",
            "Iteration 750 Loss: 0.49319741129875183\n",
            "Iteration 751 Loss: 0.49289506673812866\n",
            "Iteration 752 Loss: 0.4925941824913025\n",
            "Iteration 753 Loss: 0.4922947287559509\n",
            "Iteration 754 Loss: 0.4919968247413635\n",
            "Iteration 755 Loss: 0.49170035123825073\n",
            "Iteration 756 Loss: 0.491405189037323\n",
            "Iteration 757 Loss: 0.4911116361618042\n",
            "Iteration 758 Loss: 0.49081939458847046\n",
            "Iteration 759 Loss: 0.49052858352661133\n",
            "Iteration 760 Loss: 0.4902392029762268\n",
            "Iteration 761 Loss: 0.4899512231349945\n",
            "Iteration 762 Loss: 0.48966461420059204\n",
            "Iteration 763 Loss: 0.4893793761730194\n",
            "Iteration 764 Loss: 0.489095538854599\n",
            "Iteration 765 Loss: 0.4888130724430084\n",
            "Iteration 766 Loss: 0.48853200674057007\n",
            "Iteration 767 Loss: 0.48825228214263916\n",
            "Iteration 768 Loss: 0.4879738688468933\n",
            "Iteration 769 Loss: 0.4876967668533325\n",
            "Iteration 770 Loss: 0.48742103576660156\n",
            "Iteration 771 Loss: 0.48714661598205566\n",
            "Iteration 772 Loss: 0.4868735373020172\n",
            "Iteration 773 Loss: 0.48660174012184143\n",
            "Iteration 774 Loss: 0.4863312244415283\n",
            "Iteration 775 Loss: 0.48606204986572266\n",
            "Iteration 776 Loss: 0.4857941269874573\n",
            "Iteration 777 Loss: 0.4855274558067322\n",
            "Iteration 778 Loss: 0.48526206612586975\n",
            "Iteration 779 Loss: 0.48499804735183716\n",
            "Iteration 780 Loss: 0.4847351014614105\n",
            "Iteration 781 Loss: 0.4844735264778137\n",
            "Iteration 782 Loss: 0.4842132031917572\n",
            "Iteration 783 Loss: 0.4839540123939514\n",
            "Iteration 784 Loss: 0.4836961328983307\n",
            "Iteration 785 Loss: 0.48343947529792786\n",
            "Iteration 786 Loss: 0.48318397998809814\n",
            "Iteration 787 Loss: 0.48292970657348633\n",
            "Iteration 788 Loss: 0.4826766848564148\n",
            "Iteration 789 Loss: 0.4824247658252716\n",
            "Iteration 790 Loss: 0.4821740388870239\n",
            "Iteration 791 Loss: 0.48192450404167175\n",
            "Iteration 792 Loss: 0.4816761314868927\n",
            "Iteration 793 Loss: 0.48142898082733154\n",
            "Iteration 794 Loss: 0.48118293285369873\n",
            "Iteration 795 Loss: 0.48093801736831665\n",
            "Iteration 796 Loss: 0.4806942343711853\n",
            "Iteration 797 Loss: 0.48045164346694946\n",
            "Iteration 798 Loss: 0.4802100658416748\n",
            "Iteration 799 Loss: 0.47996971011161804\n",
            "Iteration 800 Loss: 0.47973042726516724\n",
            "Iteration 801 Loss: 0.4794922471046448\n",
            "Iteration 802 Loss: 0.47925519943237305\n",
            "Iteration 803 Loss: 0.4790192246437073\n",
            "Iteration 804 Loss: 0.47878432273864746\n",
            "Iteration 805 Loss: 0.4785504937171936\n",
            "Iteration 806 Loss: 0.4783177673816681\n",
            "Iteration 807 Loss: 0.47808608412742615\n",
            "Iteration 808 Loss: 0.4778554141521454\n",
            "Iteration 809 Loss: 0.47762587666511536\n",
            "Iteration 810 Loss: 0.4773973822593689\n",
            "Iteration 811 Loss: 0.477169930934906\n",
            "Iteration 812 Loss: 0.4769434332847595\n",
            "Iteration 813 Loss: 0.476718008518219\n",
            "Iteration 814 Loss: 0.47649359703063965\n",
            "Iteration 815 Loss: 0.4762701988220215\n",
            "Iteration 816 Loss: 0.4760478138923645\n",
            "Iteration 817 Loss: 0.4758264124393463\n",
            "Iteration 818 Loss: 0.47560596466064453\n",
            "Iteration 819 Loss: 0.4753865897655487\n",
            "Iteration 820 Loss: 0.4751681685447693\n",
            "Iteration 821 Loss: 0.47495073080062866\n",
            "Iteration 822 Loss: 0.47473424673080444\n",
            "Iteration 823 Loss: 0.4745187759399414\n",
            "Iteration 824 Loss: 0.4743041396141052\n",
            "Iteration 825 Loss: 0.474090576171875\n",
            "Iteration 826 Loss: 0.473877876996994\n",
            "Iteration 827 Loss: 0.4736661911010742\n",
            "Iteration 828 Loss: 0.47345542907714844\n",
            "Iteration 829 Loss: 0.47324562072753906\n",
            "Iteration 830 Loss: 0.47303661704063416\n",
            "Iteration 831 Loss: 0.47282856702804565\n",
            "Iteration 832 Loss: 0.4726215898990631\n",
            "Iteration 833 Loss: 0.47241532802581787\n",
            "Iteration 834 Loss: 0.4722100496292114\n",
            "Iteration 835 Loss: 0.47200560569763184\n",
            "Iteration 836 Loss: 0.47180211544036865\n",
            "Iteration 837 Loss: 0.4715994596481323\n",
            "Iteration 838 Loss: 0.4713977575302124\n",
            "Iteration 839 Loss: 0.47119688987731934\n",
            "Iteration 840 Loss: 0.4709968566894531\n",
            "Iteration 841 Loss: 0.47079771757125854\n",
            "Iteration 842 Loss: 0.4705994129180908\n",
            "Iteration 843 Loss: 0.4704020023345947\n",
            "Iteration 844 Loss: 0.4702053666114807\n",
            "Iteration 845 Loss: 0.4700096547603607\n",
            "Iteration 846 Loss: 0.4698147177696228\n",
            "Iteration 847 Loss: 0.46962061524391174\n",
            "Iteration 848 Loss: 0.4694273769855499\n",
            "Iteration 849 Loss: 0.4692349135875702\n",
            "Iteration 850 Loss: 0.4690432548522949\n",
            "Iteration 851 Loss: 0.4688524901866913\n",
            "Iteration 852 Loss: 0.46866247057914734\n",
            "Iteration 853 Loss: 0.46847325563430786\n",
            "Iteration 854 Loss: 0.46828484535217285\n",
            "Iteration 855 Loss: 0.4680972099304199\n",
            "Iteration 856 Loss: 0.46791037917137146\n",
            "Iteration 857 Loss: 0.4677243232727051\n",
            "Iteration 858 Loss: 0.4675390124320984\n",
            "Iteration 859 Loss: 0.46735450625419617\n",
            "Iteration 860 Loss: 0.46717074513435364\n",
            "Iteration 861 Loss: 0.4669877588748932\n",
            "Iteration 862 Loss: 0.4668055772781372\n",
            "Iteration 863 Loss: 0.46662408113479614\n",
            "Iteration 864 Loss: 0.46644335985183716\n",
            "Iteration 865 Loss: 0.4662633538246155\n",
            "Iteration 866 Loss: 0.4660840630531311\n",
            "Iteration 867 Loss: 0.4659056067466736\n",
            "Iteration 868 Loss: 0.465727835893631\n",
            "Iteration 869 Loss: 0.4655507504940033\n",
            "Iteration 870 Loss: 0.4653744101524353\n",
            "Iteration 871 Loss: 0.4651987850666046\n",
            "Iteration 872 Loss: 0.46502387523651123\n",
            "Iteration 873 Loss: 0.46484965085983276\n",
            "Iteration 874 Loss: 0.4646761417388916\n",
            "Iteration 875 Loss: 0.46450337767601013\n",
            "Iteration 876 Loss: 0.4643312096595764\n",
            "Iteration 877 Loss: 0.46415984630584717\n",
            "Iteration 878 Loss: 0.4639890789985657\n",
            "Iteration 879 Loss: 0.4638190269470215\n",
            "Iteration 880 Loss: 0.4636496901512146\n",
            "Iteration 881 Loss: 0.46348097920417786\n",
            "Iteration 882 Loss: 0.4633129835128784\n",
            "Iteration 883 Loss: 0.4631456136703491\n",
            "Iteration 884 Loss: 0.46297892928123474\n",
            "Iteration 885 Loss: 0.4628128707408905\n",
            "Iteration 886 Loss: 0.4626474976539612\n",
            "Iteration 887 Loss: 0.462482750415802\n",
            "Iteration 888 Loss: 0.46231865882873535\n",
            "Iteration 889 Loss: 0.46215522289276123\n",
            "Iteration 890 Loss: 0.46199244260787964\n",
            "Iteration 891 Loss: 0.4618302583694458\n",
            "Iteration 892 Loss: 0.4616686701774597\n",
            "Iteration 893 Loss: 0.46150779724121094\n",
            "Iteration 894 Loss: 0.4613474905490875\n",
            "Iteration 895 Loss: 0.46118780970573425\n",
            "Iteration 896 Loss: 0.4610288143157959\n",
            "Iteration 897 Loss: 0.4608703851699829\n",
            "Iteration 898 Loss: 0.4607125222682953\n",
            "Iteration 899 Loss: 0.4605552852153778\n",
            "Iteration 900 Loss: 0.46039867401123047\n",
            "Iteration 901 Loss: 0.46024268865585327\n",
            "Iteration 902 Loss: 0.46008723974227905\n",
            "Iteration 903 Loss: 0.4599323868751526\n",
            "Iteration 904 Loss: 0.4597781002521515\n",
            "Iteration 905 Loss: 0.4596244692802429\n",
            "Iteration 906 Loss: 0.45947134494781494\n",
            "Iteration 907 Loss: 0.4593188762664795\n",
            "Iteration 908 Loss: 0.459166944026947\n",
            "Iteration 909 Loss: 0.45901554822921753\n",
            "Iteration 910 Loss: 0.4588647484779358\n",
            "Iteration 911 Loss: 0.45871448516845703\n",
            "Iteration 912 Loss: 0.4585648477077484\n",
            "Iteration 913 Loss: 0.4584157466888428\n",
            "Iteration 914 Loss: 0.4582671821117401\n",
            "Iteration 915 Loss: 0.4581191837787628\n",
            "Iteration 916 Loss: 0.4579716920852661\n",
            "Iteration 917 Loss: 0.45782482624053955\n",
            "Iteration 918 Loss: 0.4576784670352936\n",
            "Iteration 919 Loss: 0.4575326442718506\n",
            "Iteration 920 Loss: 0.45738735795021057\n",
            "Iteration 921 Loss: 0.45724254846572876\n",
            "Iteration 922 Loss: 0.4570983052253723\n",
            "Iteration 923 Loss: 0.45695462822914124\n",
            "Iteration 924 Loss: 0.45681142807006836\n",
            "Iteration 925 Loss: 0.45666879415512085\n",
            "Iteration 926 Loss: 0.45652663707733154\n",
            "Iteration 927 Loss: 0.4563849866390228\n",
            "Iteration 928 Loss: 0.4562438726425171\n",
            "Iteration 929 Loss: 0.45610326528549194\n",
            "Iteration 930 Loss: 0.455963134765625\n",
            "Iteration 931 Loss: 0.45582354068756104\n",
            "Iteration 932 Loss: 0.45568448305130005\n",
            "Iteration 933 Loss: 0.4555458426475525\n",
            "Iteration 934 Loss: 0.4554077386856079\n",
            "Iteration 935 Loss: 0.4552701711654663\n",
            "Iteration 936 Loss: 0.45513302087783813\n",
            "Iteration 937 Loss: 0.45499634742736816\n",
            "Iteration 938 Loss: 0.45486021041870117\n",
            "Iteration 939 Loss: 0.4547245502471924\n",
            "Iteration 940 Loss: 0.4545893669128418\n",
            "Iteration 941 Loss: 0.454454630613327\n",
            "Iteration 942 Loss: 0.45432037115097046\n",
            "Iteration 943 Loss: 0.4541865885257721\n",
            "Iteration 944 Loss: 0.45405328273773193\n",
            "Iteration 945 Loss: 0.45392045378685\n",
            "Iteration 946 Loss: 0.45378804206848145\n",
            "Iteration 947 Loss: 0.4536561071872711\n",
            "Iteration 948 Loss: 0.4535246789455414\n",
            "Iteration 949 Loss: 0.45339369773864746\n",
            "Iteration 950 Loss: 0.45326313376426697\n",
            "Iteration 951 Loss: 0.4531330168247223\n",
            "Iteration 952 Loss: 0.4530033469200134\n",
            "Iteration 953 Loss: 0.4528741240501404\n",
            "Iteration 954 Loss: 0.45274537801742554\n",
            "Iteration 955 Loss: 0.4526170492172241\n",
            "Iteration 956 Loss: 0.4524891972541809\n",
            "Iteration 957 Loss: 0.45236173272132874\n",
            "Iteration 958 Loss: 0.45223468542099\n",
            "Iteration 959 Loss: 0.45210808515548706\n",
            "Iteration 960 Loss: 0.45198193192481995\n",
            "Iteration 961 Loss: 0.45185622572898865\n",
            "Iteration 962 Loss: 0.4517309367656708\n",
            "Iteration 963 Loss: 0.45160603523254395\n",
            "Iteration 964 Loss: 0.45148155093193054\n",
            "Iteration 965 Loss: 0.45135748386383057\n",
            "Iteration 966 Loss: 0.4512338638305664\n",
            "Iteration 967 Loss: 0.4511106610298157\n",
            "Iteration 968 Loss: 0.4509878158569336\n",
            "Iteration 969 Loss: 0.4508654475212097\n",
            "Iteration 970 Loss: 0.4507434368133545\n",
            "Iteration 971 Loss: 0.4506218731403351\n",
            "Iteration 972 Loss: 0.4505006670951843\n",
            "Iteration 973 Loss: 0.450379878282547\n",
            "Iteration 974 Loss: 0.4502594470977783\n",
            "Iteration 975 Loss: 0.45013946294784546\n",
            "Iteration 976 Loss: 0.45001983642578125\n",
            "Iteration 977 Loss: 0.44990065693855286\n",
            "Iteration 978 Loss: 0.4497818350791931\n",
            "Iteration 979 Loss: 0.4496634006500244\n",
            "Iteration 980 Loss: 0.44954532384872437\n",
            "Iteration 981 Loss: 0.44942766427993774\n",
            "Iteration 982 Loss: 0.44931039214134216\n",
            "Iteration 983 Loss: 0.44919347763061523\n",
            "Iteration 984 Loss: 0.44907695055007935\n",
            "Iteration 985 Loss: 0.4489607810974121\n",
            "Iteration 986 Loss: 0.4488449990749359\n",
            "Iteration 987 Loss: 0.44872963428497314\n",
            "Iteration 988 Loss: 0.44861459732055664\n",
            "Iteration 989 Loss: 0.4484999179840088\n",
            "Iteration 990 Loss: 0.4483855962753296\n",
            "Iteration 991 Loss: 0.4482716917991638\n",
            "Iteration 992 Loss: 0.4481580853462219\n",
            "Iteration 993 Loss: 0.44804489612579346\n",
            "Iteration 994 Loss: 0.44793200492858887\n",
            "Iteration 995 Loss: 0.4478195309638977\n",
            "Iteration 996 Loss: 0.4477073550224304\n",
            "Iteration 997 Loss: 0.44759559631347656\n",
            "Iteration 998 Loss: 0.44748419523239136\n",
            "Iteration 999 Loss: 0.44737303256988525\n",
            "Iteration 1000 Loss: 0.44726234674453735\n",
            "{'features': tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 1.]]), 'labels': tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]]), 'node_order': tensor([2, 0, 1, 0]), 'adjacency_list': tensor([[0, 1],\n",
            "        [0, 2],\n",
            "        [2, 3]]), 'edge_order': tensor([2, 2, 1])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}