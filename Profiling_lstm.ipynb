{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Profiling_lstm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyME8SPZ963PtVFVGfY8Vn1k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/somesh636/Software_Modelling_AI/blob/master/Profiling_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jPd7vxnmiwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "PyTorch Child-Sum Tree LSTM model\n",
        "See Tai et al. 2015 https://arxiv.org/abs/1503.00075 for model description.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class TreeLSTM(torch.nn.Module):\n",
        "    '''PyTorch TreeLSTM model that implements efficient batching.\n",
        "    '''\n",
        "    def __init__(self, in_features, out_features):\n",
        "        '''TreeLSTM class initializer\n",
        "        Takes in int sizes of in_features and out_features and sets up model Linear network layers.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        # bias terms are only on the W layers for efficiency\n",
        "        self.W_iou = torch.nn.Linear(self.in_features, 3 * self.out_features)\n",
        "        self.U_iou = torch.nn.Linear(self.out_features, 3 * self.out_features, bias=False)\n",
        "\n",
        "        # f terms are maintained seperate from the iou terms because they involve sums over child nodes\n",
        "        # while the iou terms do not\n",
        "        self.W_f = torch.nn.Linear(self.in_features, self.out_features)\n",
        "        self.U_f = torch.nn.Linear(self.out_features, self.out_features, bias=False)\n",
        "\n",
        "    def forward(self, features, node_order, adjacency_list, edge_order):\n",
        "        '''Run TreeLSTM model on a tree data structure with node features\n",
        "        Takes Tensors encoding node features, a tree node adjacency_list, and the order in which \n",
        "        the tree processing should proceed in node_order and edge_order.\n",
        "        '''\n",
        "\n",
        "        # Total number of nodes in every tree in the batch\n",
        "        batch_size = node_order.shape[0]\n",
        "\n",
        "        # Retrive device the model is currently loaded on to generate h, c, and h_sum result buffers\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # h and c states for every node in the batch\n",
        "        h = torch.zeros(batch_size, self.out_features, device=device)\n",
        "        c = torch.zeros(batch_size, self.out_features, device=device)\n",
        "\n",
        "        # populate the h and c states respecting computation order\n",
        "        for n in range(node_order.max() + 1):\n",
        "            self._run_lstm(n, h, c, features, node_order, adjacency_list, edge_order)\n",
        "\n",
        "        return h, c\n",
        "\n",
        "    def _run_lstm(self, iteration, h, c, features, node_order, adjacency_list, edge_order):\n",
        "        '''Helper function to evaluate all tree nodes currently able to be evaluated.\n",
        "        '''\n",
        "        # N is the number of nodes in the tree\n",
        "        # n is the number of nodes to be evaluated on in the current iteration\n",
        "        # E is the number of edges in the tree\n",
        "        # e is the number of edges to be evaluated on in the current iteration\n",
        "        # F is the number of features in each node\n",
        "        # M is the number of hidden neurons in the network\n",
        "\n",
        "        # node_order is a tensor of size N x 1\n",
        "        # edge_order is a tensor of size E x 1\n",
        "        # features is a tensor of size N x F\n",
        "        # adjacency_list is a tensor of size E x 2\n",
        "\n",
        "        # node_mask is a tensor of size N x 1\n",
        "        node_mask = node_order == iteration\n",
        "        # edge_mask is a tensor of size E x 1\n",
        "        edge_mask = edge_order == iteration\n",
        "\n",
        "        # x is a tensor of size n x F\n",
        "        x = features[node_mask, :]\n",
        "\n",
        "        # At iteration 0 none of the nodes should have children\n",
        "        # Otherwise, select the child nodes needed for current iteration\n",
        "        # and sum over their hidden states\n",
        "        if iteration == 0:\n",
        "            iou = self.W_iou(x)\n",
        "        else:\n",
        "            # adjacency_list is a tensor of size e x 2\n",
        "            adjacency_list = adjacency_list[edge_mask, :]\n",
        "\n",
        "            # parent_indexes and child_indexes are tensors of size e x 1\n",
        "            # parent_indexes and child_indexes contain the integer indexes needed to index into\n",
        "            # the feature and hidden state arrays to retrieve the data for those parent/child nodes.\n",
        "            parent_indexes = adjacency_list[:, 0]\n",
        "            child_indexes = adjacency_list[:, 1]\n",
        "\n",
        "            # child_h and child_c are tensors of size e x 1\n",
        "            child_h = h[child_indexes, :]\n",
        "            child_c = c[child_indexes, :]\n",
        "\n",
        "            # Add child hidden states to parent offset locations\n",
        "            _, child_counts = torch.unique_consecutive(parent_indexes, return_counts=True)\n",
        "            child_counts = tuple(child_counts)\n",
        "\n",
        "            parent_children = torch.split(child_h, child_counts)\n",
        "            parent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "            h_sum = torch.stack(parent_list)\n",
        "            iou = self.W_iou(x) + self.U_iou(h_sum)\n",
        "\n",
        "        # i, o and u are tensors of size n x M\n",
        "        i, o, u = torch.split(iou, iou.size(1) // 3, dim=1)\n",
        "        i = torch.sigmoid(i)\n",
        "        o = torch.sigmoid(o)\n",
        "        u = torch.tanh(u)\n",
        "\n",
        "        # At iteration 0 none of the nodes should have children\n",
        "        # Otherwise, calculate the forget states for each parent node and child node\n",
        "        # and sum over the child memory cell states\n",
        "        if iteration == 0:\n",
        "            c[node_mask, :] = i * u\n",
        "        else:\n",
        "            # f is a tensor of size e x M\n",
        "            f = self.W_f(features[parent_indexes, :]) + self.U_f(child_h)\n",
        "            f = torch.sigmoid(f)\n",
        "\n",
        "            # fc is a tensor of size e x M\n",
        "            fc = f * child_c\n",
        "\n",
        "            # Add the calculated f values to the parent's memory cell state\n",
        "            parent_children = torch.split(fc, child_counts)\n",
        "            parent_list = [item.sum(0) for item in parent_children]\n",
        "\n",
        "            c_sum = torch.stack(parent_list)\n",
        "            c[node_mask, :] = i * u + c_sum\n",
        "\n",
        "        h[node_mask, :] = o * torch.tanh(c[node_mask])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pRS4Nocmvv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Helper functions for running the TreeLSTM model\n",
        "'''\n",
        "\n",
        "import numpy\n",
        "import torch\n",
        "\n",
        "\n",
        "def calculate_evaluation_orders(adjacency_list, tree_size):\n",
        "    '''Calculates the node_order and edge_order from a tree adjacency_list and the tree_size.\n",
        "    The TreeLSTM model requires node_order and edge_order to be passed into the model along\n",
        "    with the node features and adjacency_list.  We pre-calculate these orders as a speed\n",
        "    optimization.\n",
        "    '''\n",
        "    adjacency_list = numpy.array(adjacency_list)\n",
        "\n",
        "    node_ids = numpy.arange(tree_size, dtype=int)\n",
        "\n",
        "    node_order = numpy.zeros(tree_size, dtype=int)\n",
        "    unevaluated_nodes = numpy.ones(tree_size, dtype=bool)\n",
        "\n",
        "    parent_nodes = adjacency_list[:, 0]\n",
        "    child_nodes = adjacency_list[:, 1]\n",
        "\n",
        "    n = 0\n",
        "    while unevaluated_nodes.any():\n",
        "        # Find which child nodes have not been evaluated\n",
        "        unevaluated_mask = unevaluated_nodes[child_nodes]\n",
        "\n",
        "        # Find the parent nodes of unevaluated children\n",
        "        unready_parents = parent_nodes[unevaluated_mask]\n",
        "\n",
        "        # Mark nodes that have not yet been evaluated\n",
        "        # and which are not in the list of parents with unevaluated child nodes\n",
        "        nodes_to_evaluate = unevaluated_nodes & ~numpy.isin(node_ids, unready_parents)\n",
        "\n",
        "        node_order[nodes_to_evaluate] = n\n",
        "        unevaluated_nodes[nodes_to_evaluate] = False\n",
        "\n",
        "        n += 1\n",
        "\n",
        "    edge_order = node_order[parent_nodes]\n",
        "\n",
        "    return node_order, edge_order\n",
        "\n",
        "\n",
        "def batch_tree_input(batch):\n",
        "    '''Combines a batch of tree dictionaries into a single batched dictionary for use by the TreeLSTM model.\n",
        "    batch - list of dicts with keys ('features', 'node_order', 'edge_order', 'adjacency_list')\n",
        "    returns a dict with keys ('features', 'node_order', 'edge_order', 'adjacency_list', 'tree_sizes')\n",
        "    '''\n",
        "    tree_sizes = [b['features'].shape[0] for b in batch]\n",
        "\n",
        "    batched_features = torch.cat([b['features'] for b in batch])\n",
        "    batched_node_order = torch.cat([b['node_order'] for b in batch])\n",
        "    batched_edge_order = torch.cat([b['edge_order'] for b in batch])\n",
        "\n",
        "    batched_adjacency_list = []\n",
        "    offset = 0\n",
        "    for n, b in zip(tree_sizes, batch):\n",
        "        batched_adjacency_list.append(b['adjacency_list'] + offset)\n",
        "        offset += n\n",
        "    batched_adjacency_list = torch.cat(batched_adjacency_list)\n",
        "\n",
        "    return {\n",
        "        'features': batched_features,\n",
        "        'node_order': batched_node_order,\n",
        "        'edge_order': batched_edge_order,\n",
        "        'adjacency_list': batched_adjacency_list,\n",
        "        'tree_sizes': tree_sizes\n",
        "    }\n",
        "\n",
        "\n",
        "def unbatch_tree_tensor(tensor, tree_sizes):\n",
        "    '''Convenience functo to unbatch a batched tree tensor into individual tensors given an array of tree_sizes.\n",
        "    sum(tree_sizes) must equal the size of tensor's zeroth dimension.\n",
        "    '''\n",
        "    return torch.split(tensor, tree_sizes, dim=0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK4toD_Dm3_M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ea6fca3-31d0-4c22-eb31-d033d62e6d98"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "def _label_node_index(node, n=0):\n",
        "    node['index'] = n\n",
        "    for child in node['children']:\n",
        "        n += 1\n",
        "        _label_node_index(child, n)\n",
        "\n",
        "\n",
        "def _gather_node_attributes(node, key):\n",
        "    features = [node[key]]\n",
        "    for child in node['children']:\n",
        "        features.extend(_gather_node_attributes(child, key))\n",
        "    return features\n",
        "\n",
        "\n",
        "def _gather_adjacency_list(node):\n",
        "    adjacency_list = []\n",
        "    for child in node['children']:\n",
        "        adjacency_list.append([node['index'], child['index']])\n",
        "        adjacency_list.extend(_gather_adjacency_list(child))\n",
        "\n",
        "    return adjacency_list\n",
        "\n",
        "\n",
        "def convert_tree_to_tensors(tree, device=torch.device('cpu')):\n",
        "    # Label each node with its walk order to match nodes to feature tensor indexes\n",
        "    # This modifies the original tree as a side effect\n",
        "    _label_node_index(tree)\n",
        "\n",
        "    features = _gather_node_attributes(tree, 'features')\n",
        "    labels = _gather_node_attributes(tree, 'labels')\n",
        "    adjacency_list = _gather_adjacency_list(tree)\n",
        "\n",
        "    node_order, edge_order = calculate_evaluation_orders(adjacency_list, len(features))\n",
        "\n",
        "    return {\n",
        "        'features': torch.tensor(features, device=device, dtype=torch.float32),\n",
        "        'labels': torch.tensor(labels, device=device, dtype=torch.float32),\n",
        "        'node_order': torch.tensor(node_order, device=device, dtype=torch.int64),\n",
        "        'adjacency_list': torch.tensor(adjacency_list, device=device, dtype=torch.int64),\n",
        "        'edge_order': torch.tensor(edge_order, device=device, dtype=torch.int64),\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Toy example\n",
        "    tree = {\n",
        "        'features': [1, 0], 'labels': [1], 'children': [\n",
        "            {'features': [0, 1], 'labels': [0], 'children': []},\n",
        "            {'features': [0, 0], 'labels': [0], 'children': [\n",
        "                {'features': [1, 1], 'labels': [0], 'children': []}\n",
        "            ]},\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    data = convert_tree_to_tensors(tree)\n",
        "\n",
        "    model = TreeLSTM(2, 1).train()\n",
        "\n",
        "    loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    for n in range(1000):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        h, c = model(\n",
        "            data['features'],\n",
        "            data['node_order'],\n",
        "            data['adjacency_list'],\n",
        "            data['edge_order']\n",
        "        )\n",
        "\n",
        "        labels = data['labels']\n",
        "\n",
        "        loss = loss_function(h, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'Iteration {n+1} Loss: {loss}')\n",
        "    print(data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1 Loss: 0.6887363195419312\n",
            "Iteration 2 Loss: 0.6886290311813354\n",
            "Iteration 3 Loss: 0.6885221600532532\n",
            "Iteration 4 Loss: 0.6884157657623291\n",
            "Iteration 5 Loss: 0.688309907913208\n",
            "Iteration 6 Loss: 0.6882044672966003\n",
            "Iteration 7 Loss: 0.6880996227264404\n",
            "Iteration 8 Loss: 0.6879951357841492\n",
            "Iteration 9 Loss: 0.6878912448883057\n",
            "Iteration 10 Loss: 0.6877877712249756\n",
            "Iteration 11 Loss: 0.6876847743988037\n",
            "Iteration 12 Loss: 0.68758225440979\n",
            "Iteration 13 Loss: 0.687480092048645\n",
            "Iteration 14 Loss: 0.6873782873153687\n",
            "Iteration 15 Loss: 0.6872766613960266\n",
            "Iteration 16 Loss: 0.6871753334999084\n",
            "Iteration 17 Loss: 0.6870741844177246\n",
            "Iteration 18 Loss: 0.6869732141494751\n",
            "Iteration 19 Loss: 0.6868722438812256\n",
            "Iteration 20 Loss: 0.6867715120315552\n",
            "Iteration 21 Loss: 0.6866707801818848\n",
            "Iteration 22 Loss: 0.6865700483322144\n",
            "Iteration 23 Loss: 0.686469316482544\n",
            "Iteration 24 Loss: 0.6863687038421631\n",
            "Iteration 25 Loss: 0.6862680912017822\n",
            "Iteration 26 Loss: 0.6861674785614014\n",
            "Iteration 27 Loss: 0.6860669255256653\n",
            "Iteration 28 Loss: 0.6859663128852844\n",
            "Iteration 29 Loss: 0.6858658194541931\n",
            "Iteration 30 Loss: 0.6857653260231018\n",
            "Iteration 31 Loss: 0.6856648921966553\n",
            "Iteration 32 Loss: 0.685564398765564\n",
            "Iteration 33 Loss: 0.6854639649391174\n",
            "Iteration 34 Loss: 0.6853635907173157\n",
            "Iteration 35 Loss: 0.6852632761001587\n",
            "Iteration 36 Loss: 0.6851629018783569\n",
            "Iteration 37 Loss: 0.6850626468658447\n",
            "Iteration 38 Loss: 0.6849623918533325\n",
            "Iteration 39 Loss: 0.6848621368408203\n",
            "Iteration 40 Loss: 0.6847619414329529\n",
            "Iteration 41 Loss: 0.6846618056297302\n",
            "Iteration 42 Loss: 0.6845616698265076\n",
            "Iteration 43 Loss: 0.6844615340232849\n",
            "Iteration 44 Loss: 0.6843615770339966\n",
            "Iteration 45 Loss: 0.6842615604400635\n",
            "Iteration 46 Loss: 0.6841615438461304\n",
            "Iteration 47 Loss: 0.6840616464614868\n",
            "Iteration 48 Loss: 0.6839617490768433\n",
            "Iteration 49 Loss: 0.6838618516921997\n",
            "Iteration 50 Loss: 0.6837621331214905\n",
            "Iteration 51 Loss: 0.6836623549461365\n",
            "Iteration 52 Loss: 0.6835626363754272\n",
            "Iteration 53 Loss: 0.6834629774093628\n",
            "Iteration 54 Loss: 0.6833633184432983\n",
            "Iteration 55 Loss: 0.6832637786865234\n",
            "Iteration 56 Loss: 0.6831642389297485\n",
            "Iteration 57 Loss: 0.6830648183822632\n",
            "Iteration 58 Loss: 0.6829653382301331\n",
            "Iteration 59 Loss: 0.6828659772872925\n",
            "Iteration 60 Loss: 0.6827666163444519\n",
            "Iteration 61 Loss: 0.6826673746109009\n",
            "Iteration 62 Loss: 0.6825681328773499\n",
            "Iteration 63 Loss: 0.6824690103530884\n",
            "Iteration 64 Loss: 0.6823698282241821\n",
            "Iteration 65 Loss: 0.6822707653045654\n",
            "Iteration 66 Loss: 0.6821717023849487\n",
            "Iteration 67 Loss: 0.6820726990699768\n",
            "Iteration 68 Loss: 0.6819737553596497\n",
            "Iteration 69 Loss: 0.6818749308586121\n",
            "Iteration 70 Loss: 0.6817761063575745\n",
            "Iteration 71 Loss: 0.6816773414611816\n",
            "Iteration 72 Loss: 0.6815785765647888\n",
            "Iteration 73 Loss: 0.6814798712730408\n",
            "Iteration 74 Loss: 0.6813812255859375\n",
            "Iteration 75 Loss: 0.6812826991081238\n",
            "Iteration 76 Loss: 0.6811841130256653\n",
            "Iteration 77 Loss: 0.6810855865478516\n",
            "Iteration 78 Loss: 0.6809872388839722\n",
            "Iteration 79 Loss: 0.6808887720108032\n",
            "Iteration 80 Loss: 0.6807904243469238\n",
            "Iteration 81 Loss: 0.6806921362876892\n",
            "Iteration 82 Loss: 0.6805938482284546\n",
            "Iteration 83 Loss: 0.6804956197738647\n",
            "Iteration 84 Loss: 0.6803975105285645\n",
            "Iteration 85 Loss: 0.6802992820739746\n",
            "Iteration 86 Loss: 0.6802011728286743\n",
            "Iteration 87 Loss: 0.6801031827926636\n",
            "Iteration 88 Loss: 0.6800050735473633\n",
            "Iteration 89 Loss: 0.6799070835113525\n",
            "Iteration 90 Loss: 0.6798091530799866\n",
            "Iteration 91 Loss: 0.6797112226486206\n",
            "Iteration 92 Loss: 0.6796132922172546\n",
            "Iteration 93 Loss: 0.6795154809951782\n",
            "Iteration 94 Loss: 0.679417610168457\n",
            "Iteration 95 Loss: 0.6793197989463806\n",
            "Iteration 96 Loss: 0.6792219877243042\n",
            "Iteration 97 Loss: 0.6791242361068726\n",
            "Iteration 98 Loss: 0.6790264844894409\n",
            "Iteration 99 Loss: 0.6789287328720093\n",
            "Iteration 100 Loss: 0.6788311004638672\n",
            "Iteration 101 Loss: 0.6787333488464355\n",
            "Iteration 102 Loss: 0.6786356568336487\n",
            "Iteration 103 Loss: 0.6785380840301514\n",
            "Iteration 104 Loss: 0.6784403920173645\n",
            "Iteration 105 Loss: 0.6783427000045776\n",
            "Iteration 106 Loss: 0.6782450675964355\n",
            "Iteration 107 Loss: 0.6781474351882935\n",
            "Iteration 108 Loss: 0.6780498027801514\n",
            "Iteration 109 Loss: 0.677952229976654\n",
            "Iteration 110 Loss: 0.6778544783592224\n",
            "Iteration 111 Loss: 0.6777569055557251\n",
            "Iteration 112 Loss: 0.6776592135429382\n",
            "Iteration 113 Loss: 0.6775615215301514\n",
            "Iteration 114 Loss: 0.6774638295173645\n",
            "Iteration 115 Loss: 0.6773661375045776\n",
            "Iteration 116 Loss: 0.6772684454917908\n",
            "Iteration 117 Loss: 0.6771706342697144\n",
            "Iteration 118 Loss: 0.6770729422569275\n",
            "Iteration 119 Loss: 0.6769751310348511\n",
            "Iteration 120 Loss: 0.6768773794174194\n",
            "Iteration 121 Loss: 0.6767795085906982\n",
            "Iteration 122 Loss: 0.676681637763977\n",
            "Iteration 123 Loss: 0.6765837073326111\n",
            "Iteration 124 Loss: 0.6764857769012451\n",
            "Iteration 125 Loss: 0.6763877868652344\n",
            "Iteration 126 Loss: 0.6762897372245789\n",
            "Iteration 127 Loss: 0.6761916875839233\n",
            "Iteration 128 Loss: 0.676093578338623\n",
            "Iteration 129 Loss: 0.675995409488678\n",
            "Iteration 130 Loss: 0.6758971214294434\n",
            "Iteration 131 Loss: 0.6757988929748535\n",
            "Iteration 132 Loss: 0.6757005453109741\n",
            "Iteration 133 Loss: 0.67560213804245\n",
            "Iteration 134 Loss: 0.6755037307739258\n",
            "Iteration 135 Loss: 0.6754052042961121\n",
            "Iteration 136 Loss: 0.6753066182136536\n",
            "Iteration 137 Loss: 0.6752079129219055\n",
            "Iteration 138 Loss: 0.6751092076301575\n",
            "Iteration 139 Loss: 0.6750103831291199\n",
            "Iteration 140 Loss: 0.6749115586280823\n",
            "Iteration 141 Loss: 0.6748126149177551\n",
            "Iteration 142 Loss: 0.6747135519981384\n",
            "Iteration 143 Loss: 0.674614429473877\n",
            "Iteration 144 Loss: 0.6745152473449707\n",
            "Iteration 145 Loss: 0.6744158267974854\n",
            "Iteration 146 Loss: 0.6743165254592896\n",
            "Iteration 147 Loss: 0.6742170453071594\n",
            "Iteration 148 Loss: 0.6741175055503845\n",
            "Iteration 149 Loss: 0.6740179061889648\n",
            "Iteration 150 Loss: 0.6739180684089661\n",
            "Iteration 151 Loss: 0.6738182306289673\n",
            "Iteration 152 Loss: 0.6737182140350342\n",
            "Iteration 153 Loss: 0.6736181974411011\n",
            "Iteration 154 Loss: 0.6735180616378784\n",
            "Iteration 155 Loss: 0.6734178066253662\n",
            "Iteration 156 Loss: 0.6733173131942749\n",
            "Iteration 157 Loss: 0.6732169389724731\n",
            "Iteration 158 Loss: 0.6731163263320923\n",
            "Iteration 159 Loss: 0.6730155944824219\n",
            "Iteration 160 Loss: 0.6729148030281067\n",
            "Iteration 161 Loss: 0.6728138327598572\n",
            "Iteration 162 Loss: 0.6727127432823181\n",
            "Iteration 163 Loss: 0.6726115942001343\n",
            "Iteration 164 Loss: 0.6725102663040161\n",
            "Iteration 165 Loss: 0.6724088788032532\n",
            "Iteration 166 Loss: 0.6723073720932007\n",
            "Iteration 167 Loss: 0.6722056269645691\n",
            "Iteration 168 Loss: 0.6721038818359375\n",
            "Iteration 169 Loss: 0.6720019578933716\n",
            "Iteration 170 Loss: 0.6718999147415161\n",
            "Iteration 171 Loss: 0.6717976927757263\n",
            "Iteration 172 Loss: 0.6716954112052917\n",
            "Iteration 173 Loss: 0.6715929508209229\n",
            "Iteration 174 Loss: 0.6714904308319092\n",
            "Iteration 175 Loss: 0.671387791633606\n",
            "Iteration 176 Loss: 0.6712849140167236\n",
            "Iteration 177 Loss: 0.6711819171905518\n",
            "Iteration 178 Loss: 0.6710788607597351\n",
            "Iteration 179 Loss: 0.6709756255149841\n",
            "Iteration 180 Loss: 0.6708722114562988\n",
            "Iteration 181 Loss: 0.6707687377929688\n",
            "Iteration 182 Loss: 0.6706650853157043\n",
            "Iteration 183 Loss: 0.6705612540245056\n",
            "Iteration 184 Loss: 0.6704573631286621\n",
            "Iteration 185 Loss: 0.6703532934188843\n",
            "Iteration 186 Loss: 0.6702490448951721\n",
            "Iteration 187 Loss: 0.6701446771621704\n",
            "Iteration 188 Loss: 0.6700402498245239\n",
            "Iteration 189 Loss: 0.6699355840682983\n",
            "Iteration 190 Loss: 0.6698307394981384\n",
            "Iteration 191 Loss: 0.6697258353233337\n",
            "Iteration 192 Loss: 0.6696208119392395\n",
            "Iteration 193 Loss: 0.6695156097412109\n",
            "Iteration 194 Loss: 0.669410228729248\n",
            "Iteration 195 Loss: 0.6693047285079956\n",
            "Iteration 196 Loss: 0.6691989898681641\n",
            "Iteration 197 Loss: 0.6690933108329773\n",
            "Iteration 198 Loss: 0.6689872741699219\n",
            "Iteration 199 Loss: 0.6688811779022217\n",
            "Iteration 200 Loss: 0.6687749624252319\n",
            "Iteration 201 Loss: 0.6686685681343079\n",
            "Iteration 202 Loss: 0.6685620546340942\n",
            "Iteration 203 Loss: 0.6684553027153015\n",
            "Iteration 204 Loss: 0.6683484315872192\n",
            "Iteration 205 Loss: 0.6682415008544922\n",
            "Iteration 206 Loss: 0.668134331703186\n",
            "Iteration 207 Loss: 0.6680269837379456\n",
            "Iteration 208 Loss: 0.6679195761680603\n",
            "Iteration 209 Loss: 0.6678119897842407\n",
            "Iteration 210 Loss: 0.6677042245864868\n",
            "Iteration 211 Loss: 0.6675962805747986\n",
            "Iteration 212 Loss: 0.6674882173538208\n",
            "Iteration 213 Loss: 0.6673800349235535\n",
            "Iteration 214 Loss: 0.6672715544700623\n",
            "Iteration 215 Loss: 0.6671630144119263\n",
            "Iteration 216 Loss: 0.6670543551445007\n",
            "Iteration 217 Loss: 0.6669455170631409\n",
            "Iteration 218 Loss: 0.6668365597724915\n",
            "Iteration 219 Loss: 0.6667273640632629\n",
            "Iteration 220 Loss: 0.6666179895401001\n",
            "Iteration 221 Loss: 0.6665085554122925\n",
            "Iteration 222 Loss: 0.6663989424705505\n",
            "Iteration 223 Loss: 0.6662891507148743\n",
            "Iteration 224 Loss: 0.6661791801452637\n",
            "Iteration 225 Loss: 0.6660690307617188\n",
            "Iteration 226 Loss: 0.665958821773529\n",
            "Iteration 227 Loss: 0.6658483743667603\n",
            "Iteration 228 Loss: 0.6657376885414124\n",
            "Iteration 229 Loss: 0.6656269431114197\n",
            "Iteration 230 Loss: 0.6655160188674927\n",
            "Iteration 231 Loss: 0.6654049754142761\n",
            "Iteration 232 Loss: 0.6652936935424805\n",
            "Iteration 233 Loss: 0.6651822924613953\n",
            "Iteration 234 Loss: 0.665070652961731\n",
            "Iteration 235 Loss: 0.6649588942527771\n",
            "Iteration 236 Loss: 0.6648470163345337\n",
            "Iteration 237 Loss: 0.6647348999977112\n",
            "Iteration 238 Loss: 0.6646225452423096\n",
            "Iteration 239 Loss: 0.664510190486908\n",
            "Iteration 240 Loss: 0.6643975973129272\n",
            "Iteration 241 Loss: 0.6642847657203674\n",
            "Iteration 242 Loss: 0.6641718745231628\n",
            "Iteration 243 Loss: 0.6640586853027344\n",
            "Iteration 244 Loss: 0.6639453768730164\n",
            "Iteration 245 Loss: 0.6638319492340088\n",
            "Iteration 246 Loss: 0.6637182235717773\n",
            "Iteration 247 Loss: 0.6636044383049011\n",
            "Iteration 248 Loss: 0.6634904146194458\n",
            "Iteration 249 Loss: 0.6633762121200562\n",
            "Iteration 250 Loss: 0.6632617712020874\n",
            "Iteration 251 Loss: 0.6631472110748291\n",
            "Iteration 252 Loss: 0.6630325317382812\n",
            "Iteration 253 Loss: 0.6629176139831543\n",
            "Iteration 254 Loss: 0.6628025770187378\n",
            "Iteration 255 Loss: 0.6626871824264526\n",
            "Iteration 256 Loss: 0.6625717282295227\n",
            "Iteration 257 Loss: 0.6624560356140137\n",
            "Iteration 258 Loss: 0.6623401641845703\n",
            "Iteration 259 Loss: 0.6622241139411926\n",
            "Iteration 260 Loss: 0.6621078848838806\n",
            "Iteration 261 Loss: 0.6619914174079895\n",
            "Iteration 262 Loss: 0.6618747711181641\n",
            "Iteration 263 Loss: 0.6617579460144043\n",
            "Iteration 264 Loss: 0.6616409420967102\n",
            "Iteration 265 Loss: 0.661523699760437\n",
            "Iteration 266 Loss: 0.6614062190055847\n",
            "Iteration 267 Loss: 0.6612885594367981\n",
            "Iteration 268 Loss: 0.6611707210540771\n",
            "Iteration 269 Loss: 0.6610527038574219\n",
            "Iteration 270 Loss: 0.6609344482421875\n",
            "Iteration 271 Loss: 0.660815954208374\n",
            "Iteration 272 Loss: 0.6606972813606262\n",
            "Iteration 273 Loss: 0.6605783104896545\n",
            "Iteration 274 Loss: 0.6604592800140381\n",
            "Iteration 275 Loss: 0.6603399515151978\n",
            "Iteration 276 Loss: 0.6602203845977783\n",
            "Iteration 277 Loss: 0.6601006388664246\n",
            "Iteration 278 Loss: 0.6599805951118469\n",
            "Iteration 279 Loss: 0.659860372543335\n",
            "Iteration 280 Loss: 0.6597399115562439\n",
            "Iteration 281 Loss: 0.6596192121505737\n",
            "Iteration 282 Loss: 0.6594983339309692\n",
            "Iteration 283 Loss: 0.6593770980834961\n",
            "Iteration 284 Loss: 0.6592558026313782\n",
            "Iteration 285 Loss: 0.6591341495513916\n",
            "Iteration 286 Loss: 0.6590123176574707\n",
            "Iteration 287 Loss: 0.6588901281356812\n",
            "Iteration 288 Loss: 0.658767819404602\n",
            "Iteration 289 Loss: 0.6586451530456543\n",
            "Iteration 290 Loss: 0.6585223078727722\n",
            "Iteration 291 Loss: 0.658399224281311\n",
            "Iteration 292 Loss: 0.6582757830619812\n",
            "Iteration 293 Loss: 0.6581522226333618\n",
            "Iteration 294 Loss: 0.6580283045768738\n",
            "Iteration 295 Loss: 0.6579040884971619\n",
            "Iteration 296 Loss: 0.6577796936035156\n",
            "Iteration 297 Loss: 0.6576550006866455\n",
            "Iteration 298 Loss: 0.6575299501419067\n",
            "Iteration 299 Loss: 0.6574046611785889\n",
            "Iteration 300 Loss: 0.6572791337966919\n",
            "Iteration 301 Loss: 0.6571533679962158\n",
            "Iteration 302 Loss: 0.6570271849632263\n",
            "Iteration 303 Loss: 0.6569007635116577\n",
            "Iteration 304 Loss: 0.6567740440368652\n",
            "Iteration 305 Loss: 0.6566470265388489\n",
            "Iteration 306 Loss: 0.6565196514129639\n",
            "Iteration 307 Loss: 0.656391978263855\n",
            "Iteration 308 Loss: 0.656264066696167\n",
            "Iteration 309 Loss: 0.6561357975006104\n",
            "Iteration 310 Loss: 0.6560071706771851\n",
            "Iteration 311 Loss: 0.6558783054351807\n",
            "Iteration 312 Loss: 0.6557490825653076\n",
            "Iteration 313 Loss: 0.6556194424629211\n",
            "Iteration 314 Loss: 0.6554895639419556\n",
            "Iteration 315 Loss: 0.6553593277931213\n",
            "Iteration 316 Loss: 0.6552287340164185\n",
            "Iteration 317 Loss: 0.6550977230072021\n",
            "Iteration 318 Loss: 0.6549663543701172\n",
            "Iteration 319 Loss: 0.6548347473144531\n",
            "Iteration 320 Loss: 0.6547027230262756\n",
            "Iteration 321 Loss: 0.6545702815055847\n",
            "Iteration 322 Loss: 0.6544374823570251\n",
            "Iteration 323 Loss: 0.6543043255805969\n",
            "Iteration 324 Loss: 0.6541708111763\n",
            "Iteration 325 Loss: 0.6540367603302002\n",
            "Iteration 326 Loss: 0.6539024710655212\n",
            "Iteration 327 Loss: 0.6537677049636841\n",
            "Iteration 328 Loss: 0.6536325812339783\n",
            "Iteration 329 Loss: 0.6534969210624695\n",
            "Iteration 330 Loss: 0.6533609628677368\n",
            "Iteration 331 Loss: 0.6532244682312012\n",
            "Iteration 332 Loss: 0.6530876159667969\n",
            "Iteration 333 Loss: 0.6529502868652344\n",
            "Iteration 334 Loss: 0.6528124809265137\n",
            "Iteration 335 Loss: 0.6526743173599243\n",
            "Iteration 336 Loss: 0.6525356769561768\n",
            "Iteration 337 Loss: 0.652396559715271\n",
            "Iteration 338 Loss: 0.6522569060325623\n",
            "Iteration 339 Loss: 0.6521168947219849\n",
            "Iteration 340 Loss: 0.6519762873649597\n",
            "Iteration 341 Loss: 0.6518352031707764\n",
            "Iteration 342 Loss: 0.6516935229301453\n",
            "Iteration 343 Loss: 0.6515514850616455\n",
            "Iteration 344 Loss: 0.651408851146698\n",
            "Iteration 345 Loss: 0.6512657403945923\n",
            "Iteration 346 Loss: 0.6511220932006836\n",
            "Iteration 347 Loss: 0.6509779095649719\n",
            "Iteration 348 Loss: 0.6508331298828125\n",
            "Iteration 349 Loss: 0.6506878137588501\n",
            "Iteration 350 Loss: 0.6505419015884399\n",
            "Iteration 351 Loss: 0.6503955125808716\n",
            "Iteration 352 Loss: 0.6502484083175659\n",
            "Iteration 353 Loss: 0.650100827217102\n",
            "Iteration 354 Loss: 0.6499525904655457\n",
            "Iteration 355 Loss: 0.6498037576675415\n",
            "Iteration 356 Loss: 0.6496542692184448\n",
            "Iteration 357 Loss: 0.6495041847229004\n",
            "Iteration 358 Loss: 0.649353563785553\n",
            "Iteration 359 Loss: 0.6492021679878235\n",
            "Iteration 360 Loss: 0.6490501761436462\n",
            "Iteration 361 Loss: 0.6488975286483765\n",
            "Iteration 362 Loss: 0.6487441062927246\n",
            "Iteration 363 Loss: 0.6485901474952698\n",
            "Iteration 364 Loss: 0.6484354138374329\n",
            "Iteration 365 Loss: 0.6482799649238586\n",
            "Iteration 366 Loss: 0.6481238603591919\n",
            "Iteration 367 Loss: 0.6479669809341431\n",
            "Iteration 368 Loss: 0.6478093862533569\n",
            "Iteration 369 Loss: 0.6476510167121887\n",
            "Iteration 370 Loss: 0.647491991519928\n",
            "Iteration 371 Loss: 0.6473321318626404\n",
            "Iteration 372 Loss: 0.6471714973449707\n",
            "Iteration 373 Loss: 0.6470100283622742\n",
            "Iteration 374 Loss: 0.6468478441238403\n",
            "Iteration 375 Loss: 0.6466848850250244\n",
            "Iteration 376 Loss: 0.6465209722518921\n",
            "Iteration 377 Loss: 0.6463562846183777\n",
            "Iteration 378 Loss: 0.6461908221244812\n",
            "Iteration 379 Loss: 0.6460243463516235\n",
            "Iteration 380 Loss: 0.6458572149276733\n",
            "Iteration 381 Loss: 0.6456891298294067\n",
            "Iteration 382 Loss: 0.6455200910568237\n",
            "Iteration 383 Loss: 0.6453501582145691\n",
            "Iteration 384 Loss: 0.6451793313026428\n",
            "Iteration 385 Loss: 0.6450076103210449\n",
            "Iteration 386 Loss: 0.6448349952697754\n",
            "Iteration 387 Loss: 0.6446613073348999\n",
            "Iteration 388 Loss: 0.6444867849349976\n",
            "Iteration 389 Loss: 0.6443111896514893\n",
            "Iteration 390 Loss: 0.6441346406936646\n",
            "Iteration 391 Loss: 0.6439571380615234\n",
            "Iteration 392 Loss: 0.6437785625457764\n",
            "Iteration 393 Loss: 0.6435990333557129\n",
            "Iteration 394 Loss: 0.6434184312820435\n",
            "Iteration 395 Loss: 0.6432367563247681\n",
            "Iteration 396 Loss: 0.6430541276931763\n",
            "Iteration 397 Loss: 0.642870306968689\n",
            "Iteration 398 Loss: 0.6426854729652405\n",
            "Iteration 399 Loss: 0.642499566078186\n",
            "Iteration 400 Loss: 0.6423124670982361\n",
            "Iteration 401 Loss: 0.6421242952346802\n",
            "Iteration 402 Loss: 0.6419349908828735\n",
            "Iteration 403 Loss: 0.6417446136474609\n",
            "Iteration 404 Loss: 0.6415529251098633\n",
            "Iteration 405 Loss: 0.6413602232933044\n",
            "Iteration 406 Loss: 0.6411662101745605\n",
            "Iteration 407 Loss: 0.6409711837768555\n",
            "Iteration 408 Loss: 0.6407748460769653\n",
            "Iteration 409 Loss: 0.6405771970748901\n",
            "Iteration 410 Loss: 0.640378475189209\n",
            "Iteration 411 Loss: 0.6401784420013428\n",
            "Iteration 412 Loss: 0.6399770975112915\n",
            "Iteration 413 Loss: 0.6397746205329895\n",
            "Iteration 414 Loss: 0.6395708322525024\n",
            "Iteration 415 Loss: 0.6393656730651855\n",
            "Iteration 416 Loss: 0.6391592621803284\n",
            "Iteration 417 Loss: 0.6389515399932861\n",
            "Iteration 418 Loss: 0.6387425065040588\n",
            "Iteration 419 Loss: 0.6385321617126465\n",
            "Iteration 420 Loss: 0.6383204460144043\n",
            "Iteration 421 Loss: 0.638107419013977\n",
            "Iteration 422 Loss: 0.6378929018974304\n",
            "Iteration 423 Loss: 0.6376771926879883\n",
            "Iteration 424 Loss: 0.6374599933624268\n",
            "Iteration 425 Loss: 0.6372414231300354\n",
            "Iteration 426 Loss: 0.6370213627815247\n",
            "Iteration 427 Loss: 0.6368000507354736\n",
            "Iteration 428 Loss: 0.6365771889686584\n",
            "Iteration 429 Loss: 0.6363529562950134\n",
            "Iteration 430 Loss: 0.636127233505249\n",
            "Iteration 431 Loss: 0.63590008020401\n",
            "Iteration 432 Loss: 0.6356714963912964\n",
            "Iteration 433 Loss: 0.6354414224624634\n",
            "Iteration 434 Loss: 0.635209858417511\n",
            "Iteration 435 Loss: 0.634976863861084\n",
            "Iteration 436 Loss: 0.634742259979248\n",
            "Iteration 437 Loss: 0.6345062851905823\n",
            "Iteration 438 Loss: 0.6342687606811523\n",
            "Iteration 439 Loss: 0.6340296268463135\n",
            "Iteration 440 Loss: 0.6337890625\n",
            "Iteration 441 Loss: 0.6335470080375671\n",
            "Iteration 442 Loss: 0.6333034038543701\n",
            "Iteration 443 Loss: 0.6330581903457642\n",
            "Iteration 444 Loss: 0.6328115463256836\n",
            "Iteration 445 Loss: 0.6325632333755493\n",
            "Iteration 446 Loss: 0.6323134303092957\n",
            "Iteration 447 Loss: 0.6320620775222778\n",
            "Iteration 448 Loss: 0.6318091154098511\n",
            "Iteration 449 Loss: 0.6315546035766602\n",
            "Iteration 450 Loss: 0.6312984824180603\n",
            "Iteration 451 Loss: 0.6310409307479858\n",
            "Iteration 452 Loss: 0.6307816505432129\n",
            "Iteration 453 Loss: 0.6305208206176758\n",
            "Iteration 454 Loss: 0.6302585005760193\n",
            "Iteration 455 Loss: 0.6299945116043091\n",
            "Iteration 456 Loss: 0.6297289729118347\n",
            "Iteration 457 Loss: 0.6294617652893066\n",
            "Iteration 458 Loss: 0.6291930675506592\n",
            "Iteration 459 Loss: 0.628922700881958\n",
            "Iteration 460 Loss: 0.6286507844924927\n",
            "Iteration 461 Loss: 0.6283773183822632\n",
            "Iteration 462 Loss: 0.6281022429466248\n",
            "Iteration 463 Loss: 0.6278254985809326\n",
            "Iteration 464 Loss: 0.6275472640991211\n",
            "Iteration 465 Loss: 0.6272674202919006\n",
            "Iteration 466 Loss: 0.6269859671592712\n",
            "Iteration 467 Loss: 0.6267029047012329\n",
            "Iteration 468 Loss: 0.6264183521270752\n",
            "Iteration 469 Loss: 0.6261321306228638\n",
            "Iteration 470 Loss: 0.6258443593978882\n",
            "Iteration 471 Loss: 0.6255550384521484\n",
            "Iteration 472 Loss: 0.6252641677856445\n",
            "Iteration 473 Loss: 0.6249716281890869\n",
            "Iteration 474 Loss: 0.6246776580810547\n",
            "Iteration 475 Loss: 0.6243820190429688\n",
            "Iteration 476 Loss: 0.6240849494934082\n",
            "Iteration 477 Loss: 0.6237862706184387\n",
            "Iteration 478 Loss: 0.6234860420227051\n",
            "Iteration 479 Loss: 0.6231842637062073\n",
            "Iteration 480 Loss: 0.6228809356689453\n",
            "Iteration 481 Loss: 0.622576117515564\n",
            "Iteration 482 Loss: 0.6222698092460632\n",
            "Iteration 483 Loss: 0.6219619512557983\n",
            "Iteration 484 Loss: 0.6216526031494141\n",
            "Iteration 485 Loss: 0.6213417649269104\n",
            "Iteration 486 Loss: 0.6210293769836426\n",
            "Iteration 487 Loss: 0.6207155585289001\n",
            "Iteration 488 Loss: 0.6204003095626831\n",
            "Iteration 489 Loss: 0.6200835704803467\n",
            "Iteration 490 Loss: 0.6197653412818909\n",
            "Iteration 491 Loss: 0.6194456815719604\n",
            "Iteration 492 Loss: 0.6191245913505554\n",
            "Iteration 493 Loss: 0.6188020706176758\n",
            "Iteration 494 Loss: 0.6184781789779663\n",
            "Iteration 495 Loss: 0.6181527376174927\n",
            "Iteration 496 Loss: 0.617825984954834\n",
            "Iteration 497 Loss: 0.6174978613853455\n",
            "Iteration 498 Loss: 0.6171683073043823\n",
            "Iteration 499 Loss: 0.6168373823165894\n",
            "Iteration 500 Loss: 0.6165051460266113\n",
            "Iteration 501 Loss: 0.6161715388298035\n",
            "Iteration 502 Loss: 0.6158366203308105\n",
            "Iteration 503 Loss: 0.6155003309249878\n",
            "Iteration 504 Loss: 0.61516273021698\n",
            "Iteration 505 Loss: 0.6148238182067871\n",
            "Iteration 506 Loss: 0.6144835948944092\n",
            "Iteration 507 Loss: 0.6141421794891357\n",
            "Iteration 508 Loss: 0.6137993335723877\n",
            "Iteration 509 Loss: 0.6134554147720337\n",
            "Iteration 510 Loss: 0.6131101846694946\n",
            "Iteration 511 Loss: 0.6127636432647705\n",
            "Iteration 512 Loss: 0.6124159097671509\n",
            "Iteration 513 Loss: 0.6120669841766357\n",
            "Iteration 514 Loss: 0.6117168664932251\n",
            "Iteration 515 Loss: 0.6113656163215637\n",
            "Iteration 516 Loss: 0.6110131144523621\n",
            "Iteration 517 Loss: 0.6106594800949097\n",
            "Iteration 518 Loss: 0.6103047132492065\n",
            "Iteration 519 Loss: 0.6099487543106079\n",
            "Iteration 520 Loss: 0.6095917224884033\n",
            "Iteration 521 Loss: 0.6092336177825928\n",
            "Iteration 522 Loss: 0.6088743209838867\n",
            "Iteration 523 Loss: 0.6085140109062195\n",
            "Iteration 524 Loss: 0.6081526279449463\n",
            "Iteration 525 Loss: 0.6077901721000671\n",
            "Iteration 526 Loss: 0.607426643371582\n",
            "Iteration 527 Loss: 0.6070621609687805\n",
            "Iteration 528 Loss: 0.606696605682373\n",
            "Iteration 529 Loss: 0.6063300967216492\n",
            "Iteration 530 Loss: 0.6059625744819641\n",
            "Iteration 531 Loss: 0.6055940389633179\n",
            "Iteration 532 Loss: 0.605224609375\n",
            "Iteration 533 Loss: 0.604854166507721\n",
            "Iteration 534 Loss: 0.6044828295707703\n",
            "Iteration 535 Loss: 0.604110598564148\n",
            "Iteration 536 Loss: 0.6037373542785645\n",
            "Iteration 537 Loss: 0.6033633947372437\n",
            "Iteration 538 Loss: 0.6029883623123169\n",
            "Iteration 539 Loss: 0.6026126146316528\n",
            "Iteration 540 Loss: 0.6022359132766724\n",
            "Iteration 541 Loss: 0.6018584966659546\n",
            "Iteration 542 Loss: 0.6014801263809204\n",
            "Iteration 543 Loss: 0.6011010408401489\n",
            "Iteration 544 Loss: 0.6007211208343506\n",
            "Iteration 545 Loss: 0.6003404259681702\n",
            "Iteration 546 Loss: 0.5999589562416077\n",
            "Iteration 547 Loss: 0.5995767712593079\n",
            "Iteration 548 Loss: 0.599193811416626\n",
            "Iteration 549 Loss: 0.5988101363182068\n",
            "Iteration 550 Loss: 0.5984257459640503\n",
            "Iteration 551 Loss: 0.5980406999588013\n",
            "Iteration 552 Loss: 0.5976549386978149\n",
            "Iteration 553 Loss: 0.5972684621810913\n",
            "Iteration 554 Loss: 0.5968813896179199\n",
            "Iteration 555 Loss: 0.5964937210083008\n",
            "Iteration 556 Loss: 0.5961053967475891\n",
            "Iteration 557 Loss: 0.5957164168357849\n",
            "Iteration 558 Loss: 0.5953269004821777\n",
            "Iteration 559 Loss: 0.5949367880821228\n",
            "Iteration 560 Loss: 0.5945460796356201\n",
            "Iteration 561 Loss: 0.5941548347473145\n",
            "Iteration 562 Loss: 0.593762993812561\n",
            "Iteration 563 Loss: 0.5933707356452942\n",
            "Iteration 564 Loss: 0.5929778814315796\n",
            "Iteration 565 Loss: 0.5925846099853516\n",
            "Iteration 566 Loss: 0.5921907424926758\n",
            "Iteration 567 Loss: 0.5917965173721313\n",
            "Iteration 568 Loss: 0.5914017558097839\n",
            "Iteration 569 Loss: 0.5910065770149231\n",
            "Iteration 570 Loss: 0.5906109809875488\n",
            "Iteration 571 Loss: 0.5902149081230164\n",
            "Iteration 572 Loss: 0.5898184776306152\n",
            "Iteration 573 Loss: 0.5894216299057007\n",
            "Iteration 574 Loss: 0.589024543762207\n",
            "Iteration 575 Loss: 0.5886269807815552\n",
            "Iteration 576 Loss: 0.5882290601730347\n",
            "Iteration 577 Loss: 0.5878308415412903\n",
            "Iteration 578 Loss: 0.587432324886322\n",
            "Iteration 579 Loss: 0.5870334506034851\n",
            "Iteration 580 Loss: 0.5866342782974243\n",
            "Iteration 581 Loss: 0.5862348675727844\n",
            "Iteration 582 Loss: 0.5858352184295654\n",
            "Iteration 583 Loss: 0.5854352712631226\n",
            "Iteration 584 Loss: 0.5850350856781006\n",
            "Iteration 585 Loss: 0.5846346616744995\n",
            "Iteration 586 Loss: 0.5842340588569641\n",
            "Iteration 587 Loss: 0.5838333368301392\n",
            "Iteration 588 Loss: 0.5834322571754456\n",
            "Iteration 589 Loss: 0.5830311179161072\n",
            "Iteration 590 Loss: 0.5826297998428345\n",
            "Iteration 591 Loss: 0.5822282433509827\n",
            "Iteration 592 Loss: 0.5818266868591309\n",
            "Iteration 593 Loss: 0.5814248919487\n",
            "Iteration 594 Loss: 0.5810230374336243\n",
            "Iteration 595 Loss: 0.5806211233139038\n",
            "Iteration 596 Loss: 0.5802190899848938\n",
            "Iteration 597 Loss: 0.579816997051239\n",
            "Iteration 598 Loss: 0.5794147849082947\n",
            "Iteration 599 Loss: 0.5790125727653503\n",
            "Iteration 600 Loss: 0.578610360622406\n",
            "Iteration 601 Loss: 0.5782080888748169\n",
            "Iteration 602 Loss: 0.577805757522583\n",
            "Iteration 603 Loss: 0.5774035453796387\n",
            "Iteration 604 Loss: 0.5770012736320496\n",
            "Iteration 605 Loss: 0.5765990018844604\n",
            "Iteration 606 Loss: 0.5761967897415161\n",
            "Iteration 607 Loss: 0.5757946968078613\n",
            "Iteration 608 Loss: 0.5753925442695618\n",
            "Iteration 609 Loss: 0.5749905109405518\n",
            "Iteration 610 Loss: 0.5745885968208313\n",
            "Iteration 611 Loss: 0.5741867423057556\n",
            "Iteration 612 Loss: 0.5737850666046143\n",
            "Iteration 613 Loss: 0.5733833909034729\n",
            "Iteration 614 Loss: 0.5729819536209106\n",
            "Iteration 615 Loss: 0.5725805759429932\n",
            "Iteration 616 Loss: 0.57217937707901\n",
            "Iteration 617 Loss: 0.571778416633606\n",
            "Iteration 618 Loss: 0.5713775157928467\n",
            "Iteration 619 Loss: 0.5709767937660217\n",
            "Iteration 620 Loss: 0.5705763697624207\n",
            "Iteration 621 Loss: 0.5701760649681091\n",
            "Iteration 622 Loss: 0.5697760581970215\n",
            "Iteration 623 Loss: 0.5693762302398682\n",
            "Iteration 624 Loss: 0.568976640701294\n",
            "Iteration 625 Loss: 0.5685772895812988\n",
            "Iteration 626 Loss: 0.5681781768798828\n",
            "Iteration 627 Loss: 0.5677793622016907\n",
            "Iteration 628 Loss: 0.5673808455467224\n",
            "Iteration 629 Loss: 0.5669825673103333\n",
            "Iteration 630 Loss: 0.566584587097168\n",
            "Iteration 631 Loss: 0.5661869645118713\n",
            "Iteration 632 Loss: 0.5657896995544434\n",
            "Iteration 633 Loss: 0.5653926730155945\n",
            "Iteration 634 Loss: 0.5649960041046143\n",
            "Iteration 635 Loss: 0.5645996928215027\n",
            "Iteration 636 Loss: 0.5642037391662598\n",
            "Iteration 637 Loss: 0.5638080835342407\n",
            "Iteration 638 Loss: 0.5634128451347351\n",
            "Iteration 639 Loss: 0.5630180239677429\n",
            "Iteration 640 Loss: 0.5626236200332642\n",
            "Iteration 641 Loss: 0.5622295141220093\n",
            "Iteration 642 Loss: 0.5618358850479126\n",
            "Iteration 643 Loss: 0.5614426732063293\n",
            "Iteration 644 Loss: 0.5610498189926147\n",
            "Iteration 645 Loss: 0.5606575012207031\n",
            "Iteration 646 Loss: 0.5602655410766602\n",
            "Iteration 647 Loss: 0.5598740577697754\n",
            "Iteration 648 Loss: 0.5594831109046936\n",
            "Iteration 649 Loss: 0.5590925216674805\n",
            "Iteration 650 Loss: 0.5587024688720703\n",
            "Iteration 651 Loss: 0.5583128929138184\n",
            "Iteration 652 Loss: 0.5579237937927246\n",
            "Iteration 653 Loss: 0.5575351715087891\n",
            "Iteration 654 Loss: 0.5571470856666565\n",
            "Iteration 655 Loss: 0.5567595362663269\n",
            "Iteration 656 Loss: 0.5563724637031555\n",
            "Iteration 657 Loss: 0.5559859275817871\n",
            "Iteration 658 Loss: 0.5555999279022217\n",
            "Iteration 659 Loss: 0.5552144646644592\n",
            "Iteration 660 Loss: 0.5548295974731445\n",
            "Iteration 661 Loss: 0.5544452667236328\n",
            "Iteration 662 Loss: 0.5540614128112793\n",
            "Iteration 663 Loss: 0.5536782741546631\n",
            "Iteration 664 Loss: 0.5532956123352051\n",
            "Iteration 665 Loss: 0.5529135465621948\n",
            "Iteration 666 Loss: 0.5525320768356323\n",
            "Iteration 667 Loss: 0.5521512627601624\n",
            "Iteration 668 Loss: 0.5517709851264954\n",
            "Iteration 669 Loss: 0.5513913631439209\n",
            "Iteration 670 Loss: 0.5510123372077942\n",
            "Iteration 671 Loss: 0.5506338477134705\n",
            "Iteration 672 Loss: 0.550256073474884\n",
            "Iteration 673 Loss: 0.5498789548873901\n",
            "Iteration 674 Loss: 0.549502432346344\n",
            "Iteration 675 Loss: 0.5491265654563904\n",
            "Iteration 676 Loss: 0.5487512946128845\n",
            "Iteration 677 Loss: 0.548376739025116\n",
            "Iteration 678 Loss: 0.5480027794837952\n",
            "Iteration 679 Loss: 0.5476295948028564\n",
            "Iteration 680 Loss: 0.5472570061683655\n",
            "Iteration 681 Loss: 0.546885073184967\n",
            "Iteration 682 Loss: 0.5465138554573059\n",
            "Iteration 683 Loss: 0.5461433529853821\n",
            "Iteration 684 Loss: 0.5457735061645508\n",
            "Iteration 685 Loss: 0.5454044342041016\n",
            "Iteration 686 Loss: 0.5450359582901001\n",
            "Iteration 687 Loss: 0.5446681976318359\n",
            "Iteration 688 Loss: 0.5443011522293091\n",
            "Iteration 689 Loss: 0.5439348220825195\n",
            "Iteration 690 Loss: 0.5435692667961121\n",
            "Iteration 691 Loss: 0.5432043075561523\n",
            "Iteration 692 Loss: 0.5428402423858643\n",
            "Iteration 693 Loss: 0.5424767732620239\n",
            "Iteration 694 Loss: 0.5421140789985657\n",
            "Iteration 695 Loss: 0.5417520999908447\n",
            "Iteration 696 Loss: 0.5413908958435059\n",
            "Iteration 697 Loss: 0.5410304665565491\n",
            "Iteration 698 Loss: 0.5406707525253296\n",
            "Iteration 699 Loss: 0.5403118133544922\n",
            "Iteration 700 Loss: 0.5399536490440369\n",
            "Iteration 701 Loss: 0.5395961403846741\n",
            "Iteration 702 Loss: 0.5392394661903381\n",
            "Iteration 703 Loss: 0.5388835072517395\n",
            "Iteration 704 Loss: 0.5385284423828125\n",
            "Iteration 705 Loss: 0.538174033164978\n",
            "Iteration 706 Loss: 0.5378203988075256\n",
            "Iteration 707 Loss: 0.5374675989151001\n",
            "Iteration 708 Loss: 0.5371155142784119\n",
            "Iteration 709 Loss: 0.5367643237113953\n",
            "Iteration 710 Loss: 0.536413848400116\n",
            "Iteration 711 Loss: 0.5360642075538635\n",
            "Iteration 712 Loss: 0.5357152819633484\n",
            "Iteration 713 Loss: 0.5353671908378601\n",
            "Iteration 714 Loss: 0.5350198745727539\n",
            "Iteration 715 Loss: 0.5346733927726746\n",
            "Iteration 716 Loss: 0.5343277454376221\n",
            "Iteration 717 Loss: 0.5339828729629517\n",
            "Iteration 718 Loss: 0.5336387157440186\n",
            "Iteration 719 Loss: 0.5332955121994019\n",
            "Iteration 720 Loss: 0.5329530239105225\n",
            "Iteration 721 Loss: 0.5326113700866699\n",
            "Iteration 722 Loss: 0.5322705507278442\n",
            "Iteration 723 Loss: 0.5319305658340454\n",
            "Iteration 724 Loss: 0.5315913558006287\n",
            "Iteration 725 Loss: 0.5312529802322388\n",
            "Iteration 726 Loss: 0.530915379524231\n",
            "Iteration 727 Loss: 0.5305786728858948\n",
            "Iteration 728 Loss: 0.5302428007125854\n",
            "Iteration 729 Loss: 0.529907763004303\n",
            "Iteration 730 Loss: 0.5295734405517578\n",
            "Iteration 731 Loss: 0.529240071773529\n",
            "Iteration 732 Loss: 0.5289074182510376\n",
            "Iteration 733 Loss: 0.5285757184028625\n",
            "Iteration 734 Loss: 0.5282447934150696\n",
            "Iteration 735 Loss: 0.5279147028923035\n",
            "Iteration 736 Loss: 0.5275854468345642\n",
            "Iteration 737 Loss: 0.5272570252418518\n",
            "Iteration 738 Loss: 0.5269293785095215\n",
            "Iteration 739 Loss: 0.5266026258468628\n",
            "Iteration 740 Loss: 0.5262768268585205\n",
            "Iteration 741 Loss: 0.5259517431259155\n",
            "Iteration 742 Loss: 0.5256274938583374\n",
            "Iteration 743 Loss: 0.5253040790557861\n",
            "Iteration 744 Loss: 0.5249815583229065\n",
            "Iteration 745 Loss: 0.5246598720550537\n",
            "Iteration 746 Loss: 0.5243390798568726\n",
            "Iteration 747 Loss: 0.5240190029144287\n",
            "Iteration 748 Loss: 0.5236998200416565\n",
            "Iteration 749 Loss: 0.5233815312385559\n",
            "Iteration 750 Loss: 0.5230640172958374\n",
            "Iteration 751 Loss: 0.5227473378181458\n",
            "Iteration 752 Loss: 0.5224316120147705\n",
            "Iteration 753 Loss: 0.5221166610717773\n",
            "Iteration 754 Loss: 0.5218026041984558\n",
            "Iteration 755 Loss: 0.5214893221855164\n",
            "Iteration 756 Loss: 0.5211769342422485\n",
            "Iteration 757 Loss: 0.5208653807640076\n",
            "Iteration 758 Loss: 0.5205546617507935\n",
            "Iteration 759 Loss: 0.520244836807251\n",
            "Iteration 760 Loss: 0.5199358463287354\n",
            "Iteration 761 Loss: 0.5196276307106018\n",
            "Iteration 762 Loss: 0.5193203687667847\n",
            "Iteration 763 Loss: 0.5190138816833496\n",
            "Iteration 764 Loss: 0.5187082290649414\n",
            "Iteration 765 Loss: 0.5184034109115601\n",
            "Iteration 766 Loss: 0.5180994868278503\n",
            "Iteration 767 Loss: 0.5177963972091675\n",
            "Iteration 768 Loss: 0.5174941420555115\n",
            "Iteration 769 Loss: 0.5171927213668823\n",
            "Iteration 770 Loss: 0.5168921947479248\n",
            "Iteration 771 Loss: 0.5165924429893494\n",
            "Iteration 772 Loss: 0.5162935853004456\n",
            "Iteration 773 Loss: 0.5159955620765686\n",
            "Iteration 774 Loss: 0.5156983733177185\n",
            "Iteration 775 Loss: 0.5154020190238953\n",
            "Iteration 776 Loss: 0.5151064991950989\n",
            "Iteration 777 Loss: 0.5148118138313293\n",
            "Iteration 778 Loss: 0.5145180225372314\n",
            "Iteration 779 Loss: 0.5142250061035156\n",
            "Iteration 780 Loss: 0.5139328837394714\n",
            "Iteration 781 Loss: 0.5136415362358093\n",
            "Iteration 782 Loss: 0.5133510828018188\n",
            "Iteration 783 Loss: 0.5130614042282104\n",
            "Iteration 784 Loss: 0.5127726197242737\n",
            "Iteration 785 Loss: 0.512484610080719\n",
            "Iteration 786 Loss: 0.5121974945068359\n",
            "Iteration 787 Loss: 0.511911153793335\n",
            "Iteration 788 Loss: 0.5116256475448608\n",
            "Iteration 789 Loss: 0.5113409757614136\n",
            "Iteration 790 Loss: 0.5110571980476379\n",
            "Iteration 791 Loss: 0.5107741951942444\n",
            "Iteration 792 Loss: 0.5104919672012329\n",
            "Iteration 793 Loss: 0.5102106332778931\n",
            "Iteration 794 Loss: 0.5099301338195801\n",
            "Iteration 795 Loss: 0.5096504092216492\n",
            "Iteration 796 Loss: 0.5093715786933899\n",
            "Iteration 797 Loss: 0.5090934634208679\n",
            "Iteration 798 Loss: 0.5088162422180176\n",
            "Iteration 799 Loss: 0.5085397958755493\n",
            "Iteration 800 Loss: 0.5082642436027527\n",
            "Iteration 801 Loss: 0.5079894065856934\n",
            "Iteration 802 Loss: 0.5077154040336609\n",
            "Iteration 803 Loss: 0.5074422359466553\n",
            "Iteration 804 Loss: 0.5071698427200317\n",
            "Iteration 805 Loss: 0.5068983435630798\n",
            "Iteration 806 Loss: 0.50662761926651\n",
            "Iteration 807 Loss: 0.5063576698303223\n",
            "Iteration 808 Loss: 0.5060885548591614\n",
            "Iteration 809 Loss: 0.5058202743530273\n",
            "Iteration 810 Loss: 0.5055527091026306\n",
            "Iteration 811 Loss: 0.5052859783172607\n",
            "Iteration 812 Loss: 0.505020022392273\n",
            "Iteration 813 Loss: 0.504754900932312\n",
            "Iteration 814 Loss: 0.5044906139373779\n",
            "Iteration 815 Loss: 0.5042270421981812\n",
            "Iteration 816 Loss: 0.5039643049240112\n",
            "Iteration 817 Loss: 0.5037024021148682\n",
            "Iteration 818 Loss: 0.5034412145614624\n",
            "Iteration 819 Loss: 0.5031808614730835\n",
            "Iteration 820 Loss: 0.5029212832450867\n",
            "Iteration 821 Loss: 0.5026624798774719\n",
            "Iteration 822 Loss: 0.502404510974884\n",
            "Iteration 823 Loss: 0.5021472573280334\n",
            "Iteration 824 Loss: 0.5018907785415649\n",
            "Iteration 825 Loss: 0.5016350746154785\n",
            "Iteration 826 Loss: 0.5013801455497742\n",
            "Iteration 827 Loss: 0.5011261105537415\n",
            "Iteration 828 Loss: 0.5008727312088013\n",
            "Iteration 829 Loss: 0.5006201863288879\n",
            "Iteration 830 Loss: 0.5003683567047119\n",
            "Iteration 831 Loss: 0.500117301940918\n",
            "Iteration 832 Loss: 0.4998670220375061\n",
            "Iteration 833 Loss: 0.49961745738983154\n",
            "Iteration 834 Loss: 0.49936872720718384\n",
            "Iteration 835 Loss: 0.4991207420825958\n",
            "Iteration 836 Loss: 0.4988735318183899\n",
            "Iteration 837 Loss: 0.49862703680992126\n",
            "Iteration 838 Loss: 0.4983813166618347\n",
            "Iteration 839 Loss: 0.49813640117645264\n",
            "Iteration 840 Loss: 0.4978921413421631\n",
            "Iteration 841 Loss: 0.4976486563682556\n",
            "Iteration 842 Loss: 0.49740591645240784\n",
            "Iteration 843 Loss: 0.49716395139694214\n",
            "Iteration 844 Loss: 0.49692273139953613\n",
            "Iteration 845 Loss: 0.4966822862625122\n",
            "Iteration 846 Loss: 0.4964424967765808\n",
            "Iteration 847 Loss: 0.4962034821510315\n",
            "Iteration 848 Loss: 0.4959651827812195\n",
            "Iteration 849 Loss: 0.49572765827178955\n",
            "Iteration 850 Loss: 0.4954908490180969\n",
            "Iteration 851 Loss: 0.4952547550201416\n",
            "Iteration 852 Loss: 0.49501940608024597\n",
            "Iteration 853 Loss: 0.49478477239608765\n",
            "Iteration 854 Loss: 0.4945509135723114\n",
            "Iteration 855 Loss: 0.4943176805973053\n",
            "Iteration 856 Loss: 0.4940852224826813\n",
            "Iteration 857 Loss: 0.49385350942611694\n",
            "Iteration 858 Loss: 0.49362245202064514\n",
            "Iteration 859 Loss: 0.49339213967323303\n",
            "Iteration 860 Loss: 0.4931625723838806\n",
            "Iteration 861 Loss: 0.49293363094329834\n",
            "Iteration 862 Loss: 0.49270546436309814\n",
            "Iteration 863 Loss: 0.49247801303863525\n",
            "Iteration 864 Loss: 0.4922512471675873\n",
            "Iteration 865 Loss: 0.4920251965522766\n",
            "Iteration 866 Loss: 0.4917997717857361\n",
            "Iteration 867 Loss: 0.49157509207725525\n",
            "Iteration 868 Loss: 0.4913511276245117\n",
            "Iteration 869 Loss: 0.4911278486251831\n",
            "Iteration 870 Loss: 0.4909052550792694\n",
            "Iteration 871 Loss: 0.49068331718444824\n",
            "Iteration 872 Loss: 0.490462064743042\n",
            "Iteration 873 Loss: 0.49024155735969543\n",
            "Iteration 874 Loss: 0.4900217056274414\n",
            "Iteration 875 Loss: 0.4898025691509247\n",
            "Iteration 876 Loss: 0.4895841181278229\n",
            "Iteration 877 Loss: 0.4893662631511688\n",
            "Iteration 878 Loss: 0.4891491234302521\n",
            "Iteration 879 Loss: 0.48893266916275024\n",
            "Iteration 880 Loss: 0.48871687054634094\n",
            "Iteration 881 Loss: 0.48850172758102417\n",
            "Iteration 882 Loss: 0.4882872998714447\n",
            "Iteration 883 Loss: 0.48807352781295776\n",
            "Iteration 884 Loss: 0.48786038160324097\n",
            "Iteration 885 Loss: 0.4876478910446167\n",
            "Iteration 886 Loss: 0.48743608593940735\n",
            "Iteration 887 Loss: 0.4872249960899353\n",
            "Iteration 888 Loss: 0.48701441287994385\n",
            "Iteration 889 Loss: 0.48680463433265686\n",
            "Iteration 890 Loss: 0.4865954518318176\n",
            "Iteration 891 Loss: 0.48638686537742615\n",
            "Iteration 892 Loss: 0.486178994178772\n",
            "Iteration 893 Loss: 0.48597174882888794\n",
            "Iteration 894 Loss: 0.48576509952545166\n",
            "Iteration 895 Loss: 0.4855591654777527\n",
            "Iteration 896 Loss: 0.48535382747650146\n",
            "Iteration 897 Loss: 0.4851491153240204\n",
            "Iteration 898 Loss: 0.48494505882263184\n",
            "Iteration 899 Loss: 0.4847416281700134\n",
            "Iteration 900 Loss: 0.48453882336616516\n",
            "Iteration 901 Loss: 0.48433661460876465\n",
            "Iteration 902 Loss: 0.48413506150245667\n",
            "Iteration 903 Loss: 0.4839341938495636\n",
            "Iteration 904 Loss: 0.4837338328361511\n",
            "Iteration 905 Loss: 0.48353418707847595\n",
            "Iteration 906 Loss: 0.48333510756492615\n",
            "Iteration 907 Loss: 0.4831366539001465\n",
            "Iteration 908 Loss: 0.4829387962818146\n",
            "Iteration 909 Loss: 0.4827415645122528\n",
            "Iteration 910 Loss: 0.4825449287891388\n",
            "Iteration 911 Loss: 0.4823489189147949\n",
            "Iteration 912 Loss: 0.4821535348892212\n",
            "Iteration 913 Loss: 0.4819587469100952\n",
            "Iteration 914 Loss: 0.4817644953727722\n",
            "Iteration 915 Loss: 0.48157092928886414\n",
            "Iteration 916 Loss: 0.48137789964675903\n",
            "Iteration 917 Loss: 0.4811854660511017\n",
            "Iteration 918 Loss: 0.48099368810653687\n",
            "Iteration 919 Loss: 0.480802446603775\n",
            "Iteration 920 Loss: 0.48061180114746094\n",
            "Iteration 921 Loss: 0.4804217517375946\n",
            "Iteration 922 Loss: 0.480232298374176\n",
            "Iteration 923 Loss: 0.4800434112548828\n",
            "Iteration 924 Loss: 0.47985509037971497\n",
            "Iteration 925 Loss: 0.4796673655509949\n",
            "Iteration 926 Loss: 0.47948023676872253\n",
            "Iteration 927 Loss: 0.4792936444282532\n",
            "Iteration 928 Loss: 0.47910764813423157\n",
            "Iteration 929 Loss: 0.4789222180843353\n",
            "Iteration 930 Loss: 0.47873732447624207\n",
            "Iteration 931 Loss: 0.47855305671691895\n",
            "Iteration 932 Loss: 0.4783693253993988\n",
            "Iteration 933 Loss: 0.47818613052368164\n",
            "Iteration 934 Loss: 0.4780035614967346\n",
            "Iteration 935 Loss: 0.4778215289115906\n",
            "Iteration 936 Loss: 0.4776400029659271\n",
            "Iteration 937 Loss: 0.47745904326438904\n",
            "Iteration 938 Loss: 0.4772786796092987\n",
            "Iteration 939 Loss: 0.47709888219833374\n",
            "Iteration 940 Loss: 0.476919561624527\n",
            "Iteration 941 Loss: 0.47674083709716797\n",
            "Iteration 942 Loss: 0.47656264901161194\n",
            "Iteration 943 Loss: 0.4763849973678589\n",
            "Iteration 944 Loss: 0.4762079119682312\n",
            "Iteration 945 Loss: 0.4760313034057617\n",
            "Iteration 946 Loss: 0.4758552610874176\n",
            "Iteration 947 Loss: 0.47567981481552124\n",
            "Iteration 948 Loss: 0.4755048155784607\n",
            "Iteration 949 Loss: 0.4753304123878479\n",
            "Iteration 950 Loss: 0.4751565158367157\n",
            "Iteration 951 Loss: 0.4749831557273865\n",
            "Iteration 952 Loss: 0.47481033205986023\n",
            "Iteration 953 Loss: 0.4746379554271698\n",
            "Iteration 954 Loss: 0.4744661748409271\n",
            "Iteration 955 Loss: 0.47429490089416504\n",
            "Iteration 956 Loss: 0.47412413358688354\n",
            "Iteration 957 Loss: 0.47395390272140503\n",
            "Iteration 958 Loss: 0.4737841784954071\n",
            "Iteration 959 Loss: 0.47361496090888977\n",
            "Iteration 960 Loss: 0.4734462797641754\n",
            "Iteration 961 Loss: 0.4732780456542969\n",
            "Iteration 962 Loss: 0.4731103777885437\n",
            "Iteration 963 Loss: 0.47294318675994873\n",
            "Iteration 964 Loss: 0.47277650237083435\n",
            "Iteration 965 Loss: 0.47261035442352295\n",
            "Iteration 966 Loss: 0.47244465351104736\n",
            "Iteration 967 Loss: 0.47227948904037476\n",
            "Iteration 968 Loss: 0.47211483120918274\n",
            "Iteration 969 Loss: 0.4719506502151489\n",
            "Iteration 970 Loss: 0.4717869460582733\n",
            "Iteration 971 Loss: 0.4716237485408783\n",
            "Iteration 972 Loss: 0.4714610278606415\n",
            "Iteration 973 Loss: 0.47129884362220764\n",
            "Iteration 974 Loss: 0.47113707661628723\n",
            "Iteration 975 Loss: 0.4709758162498474\n",
            "Iteration 976 Loss: 0.4708150327205658\n",
            "Iteration 977 Loss: 0.47065478563308716\n",
            "Iteration 978 Loss: 0.47049498558044434\n",
            "Iteration 979 Loss: 0.47033560276031494\n",
            "Iteration 980 Loss: 0.47017672657966614\n",
            "Iteration 981 Loss: 0.47001832723617554\n",
            "Iteration 982 Loss: 0.46986040472984314\n",
            "Iteration 983 Loss: 0.46970295906066895\n",
            "Iteration 984 Loss: 0.46954602003097534\n",
            "Iteration 985 Loss: 0.46938949823379517\n",
            "Iteration 986 Loss: 0.4692334532737732\n",
            "Iteration 987 Loss: 0.4690778851509094\n",
            "Iteration 988 Loss: 0.4689227342605591\n",
            "Iteration 989 Loss: 0.46876806020736694\n",
            "Iteration 990 Loss: 0.468613862991333\n",
            "Iteration 991 Loss: 0.46846017241477966\n",
            "Iteration 992 Loss: 0.4683068096637726\n",
            "Iteration 993 Loss: 0.46815401315689087\n",
            "Iteration 994 Loss: 0.4680016040802002\n",
            "Iteration 995 Loss: 0.4678496718406677\n",
            "Iteration 996 Loss: 0.46769821643829346\n",
            "Iteration 997 Loss: 0.46754714846611023\n",
            "Iteration 998 Loss: 0.46739649772644043\n",
            "Iteration 999 Loss: 0.4672463536262512\n",
            "Iteration 1000 Loss: 0.4670966565608978\n",
            "{'features': tensor([[1., 0.],\n",
            "        [0., 1.],\n",
            "        [0., 0.],\n",
            "        [1., 1.]]), 'labels': tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]]), 'node_order': tensor([2, 0, 1, 0]), 'adjacency_list': tensor([[0, 1],\n",
            "        [0, 2],\n",
            "        [2, 3]]), 'edge_order': tensor([2, 2, 1])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}